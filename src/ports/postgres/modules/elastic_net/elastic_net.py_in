
import plpy
import re
from elastic_net_models import __elastic_net_gaussian_igd_train
from elastic_net_models import __elastic_net_gaussian_fista_train
from elastic_net_models import __elastic_net_binomial_fista_train
from elastic_net_models import __elastic_net_binomial_igd_train
from utilities.validate_args import __is_col_array
from utilities.validate_args import __is_tbl_exists
from utilities.validate_args import __is_tbl_has_rows
from utilities.validate_args import __is_col_exists
from utilities.utilities import _string_to_array
from utilities.validate_args import __get_cols
from utilities.utilities import __mad_version

version_wrapper = __mad_version()
mad_vec = version_wrapper.select_vecfunc()

# ========================================================================

def elastic_net_help(schema_madlib, family_or_optimizer = None, **kwargs):
    """
    Given a response family name or optimizer name, print out the related
    information.

    If a family name is given, print out the supported optimizer together
    with its default optimizer.
    
    If an optimizer name is given, print out the necessary parameters.
    """
    if (family_or_optimizer is None or
        family_or_optimizer.lower() == "help" or
        family_or_optimizer.lower() == "?"):
        return """
        ----------------------------------------------------------------
                                Summary
        ----------------------------------------------------------------
        Right now, gaussian (linear) and binomial (logistic) families  
        are supported!
        --
        Run:
        SELECT {schema_madlib}.elastic_net_train('gaussian');
        or
        SELECT {schema_madlib}.elastic_net_train('binomial');   
        to see more help.
        --
        Run:  SELECT {schema_madlib}.elastic_net_train('usage');   
        to see how to use.
        --
        Run:  SELECT {schema_madlib}.elastic_net_train('predict');   
        to see how to predict.
        """.format(schema_madlib = schema_madlib)

    if (family_or_optimizer.lower() == "usage" ):
        return """
        ----------------------------------------------------------------
                                Usage
        ----------------------------------------------------------------
        SELECT {schema_madlib}.elastic_net_train (
            'tbl_source',      -- Data table
            'tbl_result',      -- Result table
            'col_dep_var',     -- Dependent variable, can be an expression or
                                    '*'
            'col_ind_var',     -- Independent variable, can be an expression
            'regress_family',  -- 'gaussian' (or 'linear'). 'binomial'
                                    (or 'logistic')
            alpha,             -- Elastic net control parameter, value in [0, 1]   
            lambda_value,      -- Regularization parameter, positive
            standardize,       -- Whether to normalize the data
            'grouping_col',    -- Group by which columns. Default: NULL
            'optimizer',       -- Name of optimizer. Default: 'fista'
            'optimizer_params',-- Text array of optimizer parameters
            'excluded',        -- Column names excluded from '*'
            max_iter,          -- Maximum iteration number
            tolerance          -- Stopping criteria
        );
        ----------------------------------------------------------------
                                Ouput
        ----------------------------------------------------------------
        The output table (tbl_result in the above) has the following columns:
        family            TEXT,       -- 'gaussian' or 'binomial'
        features          TEXT[],     -- All feature column names
        features_selected TEXT[],     -- Features with non-zero coefficients
        coef_nonzero      DOUBLE PRECISION[], -- Non-zero coefficients
        coef_all          DOUBLE PRECISION[], -- All coefficients
        intercept         DOUBLE PRECISION,   -- Intercept of the linear fit
        log_likelihood    DOUBLE PRECISION,   -- log-likelihood of the fit
        standardize       BOOLEAN,    -- Whether the data was standardized
                                         before fitting
        iteration_run     INTEGER     -- How many iteration was actually run

        If the independent variable is a column with type of array, features
        and features_selected will output indices of the array.    
        """.format(schema_madlib = schema_madlib)

    if family_or_optimizer.lower() == "predict":
        return """
        ----------------------------------------------------------------
                                Prediction
        ----------------------------------------------------------------
        SELECT {schema_madlib}.elastic_net_predict(
            'regress_family', -- 'gaussian' (or 'linear'). 'binomial'
                                  (or 'logistic') will be supported
            coefficients,     -- Fitting coefficients as a double
                                 array
            intercept,
            ind_var           -- independent variables
        ) FROM tbl_result, tbl_new_source
        LIMIT 10;
        will calculate 10 fitting values.

        When predicting with binomial models, the return value is 1
        if the predicted result is True, and 0 if the prediction is
        False.

        OR -------------------------------------------

        (1) SELECT {schema_madlib}.elastic_net_gaussian_predict (
                coefficients, intercept, ind_var
            ) FROM tbl_result, tbl_new_source LIMIT 10;

        (2) SELECT {schema_madlib}.elastic_net_binomial_predict (
                coefficients, intercept, ind_var
            ) FROM tbl_result, tbl_new_source LIMIT 10;

            This returns 10 BOOLEAN values.

        (3) SELECT {schema_madlib}.elastic_net_binomial_prob (
                coefficients, intercept, ind_var
            ) FROM tbl_result, tbl_new_source LIMIT 10;

            This returns 10 probability values for True class.

        OR -------------------------------------------

        SELECT {schema_madlib}.elastic_net_predict(
            'tbl_model',      -- Result table of elastic_net_train
            'tbl_new_source', -- New data source
            'col_id',         -- Unique ID column
            'tbl_predict'     -- Prediction result
        );
        will put all prediction results into a table. This can be
        used together with cross_validation_general() function.

        When predicting with binomial models, the predicted values
        are BOOLEAN.        
        """.format(schema_madlib = schema_madlib)
        
    if (family_or_optimizer.lower() == "gaussian" or
        family_or_optimizer.lower() == "linear"):
        return """
        ----------------------------------------------------------------
        Fitting of linear models
        ----------------------------------------------------------------
        Supported optimizer:
        (1) Incremental gradient descent method ('igd')
        (2) Fast iterative shrinkage thesholding algorithm ('fista')

        Default is 'fista'
        --
        Run:
        SELECT {schema_madlib}.elastic_net_train('optimizer');
        to see more help on each optimizer.
        """.format(schema_madlib = schema_madlib)

    if (family_or_optimizer.lower() == "binomial" or
        family_or_optimizer.lower() == "logistic"):
        return """
        ----------------------------------------------------------------
        Fitting of logistic models
        ----------------------------------------------------------------
        The dependent variable must be a BOOLEAN.
        
        Supported optimizer:
        (1) Incremental gradient descent method ('igd')
        (2) Fast iterative shrinkage thesholding algorithm ('fista')
        
        Default is 'fista'
        --
        Run:
        SELECT {schema_madlib}.elastic_net_train('optimizer');
        to see more help on each optimizer.
        """.format(schema_madlib = schema_madlib)

    if family_or_optimizer.lower() == "igd":
        return """
        ----------------------------------------------------------------
        Incremental gradient descent (IGD) method
        ----------------------------------------------------------------
        Right now, it supports fitting of both linear and logistic models.
        
        In order to obtain sparse coefficients, a
        modified version of IGD is actually used.
        
        Parameters --------------------------------
        stepsize         - default is 0.01
        threshold        - default is 1e-10. When a coefficient is really
                           small, set it to be 0
        warmup           - default is False
        warmup_lambdas   - default is Null
        warmup_lambda_no - default is 15. How many lambda's are used in
                           warm-up, will be overridden if warmup_lambdas   
                           is not NULL
        warmup_tolerance - default is the same as tolerance. The value
                           of tolerance used during warmup. 
        parallel         - default is True. Run the computation on
                           multiple segments or not.

        When warmup is True and warmup_lambdas is NULL, a series
        of lambda values will be automatically generated and used.
        
        Reference --------------------------------
        [1] Shai Shalev-Shwartz and Ambuj Tewari, Stochastic Methods for l1    
            Regularized Loss Minimization. Proceedings of the 26th Interna-   
            tional Conference on Machine Learning, Montreal, Canada, 2009.   
        """

    if family_or_optimizer.lower() == "fista":
        return """
        ----------------------------------------------------------------
        Fast iterative shrinkage thesholding algorithm
        with backtracking for stepsizes
        ----------------------------------------------------------------
        Right now, it supports fitting of both linear and logistic models.
        
        Parameters --------------------------------
        max_stepsize     - default is 4.0
        eta              - default is 1.2, if stepsize does not work
                           stepsize/eta will be tried
        warmup           - default is False
        warmup_lambdas   - default is NULL, which means that lambda
                           values will be automatically generated
        warmup_lambda_no - default is 15. How many lambda's are used in
                           warm-up, will be overridden if warmup_lambdas     
                           is not NULL
        warmup_tolerance - default is the same as tolerance. The value
                           of tolerance used during warmup. 
        use_active_set   - default is False. Sometimes active-set method
                           can speed up the calculation.
        activeset_tolerance - default is the same as tolerance. The
                              value of tolerance used during active set
                              calculation
        random_stepsize - default is False. Whether add some randomness
                          to the step size. Sometimes, this can speed
                          up the calculation.

        When warmup is True and warmup_lambdas is NULL, warmup_lambda_no
        of lambda values will be automatically generated and used.
        
        Reference --------------------------------
        [1] Beck, A. and M. Teboulle (2009), A fast iterative   
            shrinkage-thresholding algorothm for linear inverse   
            problems. SIAM J. on Imaging Sciences 2(1), 183-202.   
        """
        
    # if family_or_optimizer.lower() == "newton":
    #     return "Newton method  "
        
    return """
    Elastic Net error: Not a supported response family or optimizer
    Run:
    SELECT {schema_madlib}.elastic_net_train();
    for help
    """.format(schema_madlib = schema_madlib)
        

# ========================================================================

# lambda is a keyword of Python, try to avoid using
# it, so use lambda_value instead
def elastic_net_train(schema_madlib, tbl_source, tbl_result, col_dep_var,
                      col_ind_var, regress_family, alpha, lambda_value,
                      standardize, grouping_col, optimizer,
                      optimizer_params, excluded, max_iter, tolerance,
                      **kwargs):
    """
    A wrapper for all variants of elastic net regularization.

    @param tbl_source        Name of data source table
    @param col_ind_var       Name of independent variable column,
                             independent variable is an array
    @param col_dep_var       Name of dependent variable column
    @param tbl_result        Name of the table to store the results,
                             will return fitting coefficients and
                             likelihood
    @param lambda_value      The regularization parameter
    @param alpha             The elastic net parameter, [0, 1]
    @param standardize       Whether to normalize the variables
    @param regress_family    Response type, 'gaussian' or 'binomial'
    @param optimizer         The optimization algorithm, for example 'igd'
    @param optimizer_params  Parameters of the above optimizer, the format
                             is '{arg = value, ...}'::varchar[]
    @param excluded          Which variables are excluded when
                             col_ind_var == "*"
    """
    if regress_family is None:
        plpy.error("""
                   Elastic Net error: Please enter a valid response family name!
                   Run:
                   SELECT {schema_madlib}.elastic_net_train();
                   for supported response family.
                   """.format(schema_madlib = schema_madlib))

    if optimizer is None:
        plpy.error("""
                   Elastic Net error: Please enter a valid optimizer name!
                   Run:
                   SELECT {schema_madlib}.elastic_net_train('gaussian');
                   for supported optimizers.
                   """.format(schema_madlib = schema_madlib))
        
    # handle all special cases of col_ind_var
    (col_ind_var, outstr_array) = analyze_input_str(schema_madlib, tbl_source, col_ind_var, 
                                                    col_dep_var, excluded)
    # # Special case for ridge linear regression
    # if ((regress_family.lower() == "gaussian" or regress_family.lower() == "linear") and
    #     optimizer.lower() == "newton" and
    #     alpha == 0):
    #     plpy.execute("""select {schema_madlib}.ridge_newton_train(
    #                         '{tbl_source}', '{col_ind_var}', '{col_dep_var}',
    #                         '{tbl_result}', {lambda_value}, {standardize}
    #     )""".format(schema_madlib = schema_madlib,
    #                 tbl_source = tbl_source,
    #                 col_ind_var = col_ind_var,
    #                 col_dep_var = col_dep_var,
    #                 tbl_result = tbl_result,
    #                 lambda_value = lambda_value,
    #                 standardize = standardize))
    #     return None
    
    if ((regress_family.lower() == "gaussian" or regress_family.lower() == "linear") and
        optimizer.lower() == "igd"):
        __elastic_net_gaussian_igd_train(schema_madlib, tbl_source, col_ind_var,
                                         col_dep_var, tbl_result, lambda_value, alpha,
                                         standardize, optimizer_params, max_iter,
                                         tolerance, outstr_array, **kwargs)
        return None

    if ((regress_family.lower() == "gaussian" or regress_family.lower() == "linear") and
        optimizer.lower() == "fista"):
        __elastic_net_gaussian_fista_train(schema_madlib, tbl_source, col_ind_var,
                                           col_dep_var, tbl_result, lambda_value, alpha,
                                           standardize, optimizer_params, max_iter,
                                           tolerance, outstr_array, **kwargs)
        return None

    if ((regress_family.lower() == "binomial" or regress_family.lower() == "logistic") and
        optimizer.lower() == "igd"):
        __elastic_net_binomial_igd_train(schema_madlib, tbl_source, col_ind_var,
                                         col_dep_var, tbl_result, lambda_value, alpha,
                                         standardize, optimizer_params, max_iter,
                                         tolerance, outstr_array, **kwargs)
        return None

    if ((regress_family.lower() == "binomial" or regress_family.lower() == "logistic") and
        optimizer.lower() == "fista"):
        __elastic_net_binomial_fista_train(schema_madlib, tbl_source, col_ind_var,
                                           col_dep_var, tbl_result, lambda_value, alpha,
                                           standardize, optimizer_params, max_iter,
                                           tolerance, outstr_array, **kwargs)
        return None

    plpy.error("""
               Elastic Net error: Not a supported response family or supported optimizer of the given response family!
               Run:
               SELECT {schema_madlib}.elastic_net_train();
               for help.
               """.format(schema_madlib = schema_madlib))
    return None

# ========================================================================

def __check_args (tbl_source, col_ind_var, col_dep_var):
    """
    Check arguments before analyze_input_str
    """
    if (tbl_source is None or col_ind_var is None or col_dep_var is None):
        plpy.error("Elastic Net error: You have unsupported NULL value(s) in the arguments!")
    
    if not __is_tbl_exists(tbl_source):
        plpy.error("Elastic Net error: Data table " + tbl_source + " does not exist!")

    if not __is_tbl_has_rows(tbl_source):
        plpy.error("Elastic Net error: Data table " + tbl_source + " is empty!")
    
# ========================================================================
    
def analyze_input_str(schema_madlib, tbl_source,
                      col_ind_var, col_dep_var, excluded):
    """
    Make input strings and output strings compatible with functions

    @param tbl_source Data table
    @param col_ind_var Independent variables
    @param col_dep_var Dependent variables
    @param excluded Which variables are excluded when col_ind_var == "*"
    """
    __check_args(tbl_source, col_ind_var, col_dep_var)
    
    outstr_array = []
    
    if col_ind_var == "*":
        cols = __get_cols(tbl_source, schema_madlib)
        if excluded is not None:
            s = _string_to_array(excluded)
        else:
            s = []

        for i in range(len(cols)):
            if cols[i] not in s and cols[i] != col_dep_var:
                outstr_array.append(cols[i])

        col_ind_var_new = "array["
        for i in range(len(outstr_array)):
            col_ind_var_new += outstr_array[i]
            if i != len(outstr_array) - 1:
                col_ind_var_new += ", "
        col_ind_var_new += "]"
        return (col_ind_var_new, outstr_array)

    if __is_col_exists(tbl_source, [col_ind_var], schema_madlib):
        # a single column is independent variable
        # which means that it is an array
        # excluded must be a string contains integers
        if not __is_col_array(tbl_source, col_ind_var):
            plpy.error("Elastic Net error: The independent variable must be an array!")
        
        if excluded is not None:
            s = _string_to_array(excluded)
            try:
                for i in range(len(s)):
                    s[i] = int(s[i])
            except:
                plpy.error("Elastic Net error: When the independent variable is an array, excluded can only be integers!")
        else:
            s = []

        dimension = plpy.execute(
            """
            select array_upper({col_ind_var},1) as dimension
            from {tbl_source} limit 1
            """.format(tbl_source = tbl_source,
                       col_ind_var = col_ind_var))[0]["dimension"]

        if excluded is None:
            col_ind_var_new = col_ind_var               
            outstr_array = map(lambda x: col_ind_var + "[" + str(x + 1) + "]",
                               range(dimension))
        else:
            col_ind_var_new = "array["
            for i in range(dimension):
                if i+1 not in s:
                    col_ind_var_new += col_ind_var + "[" + str(i+1) + "]"
                    outstr_array.append(col_ind_var + "[" + str(i+1) + "]")
                    if i != dimension - 1:
                        col_ind_var_new += ", "
            col_ind_var_new += "]"
                        
        return (col_ind_var_new, outstr_array)
            
    try:
        matched = re.match(r"(?i)^array\[(.*)\]", col_ind_var)
        if matched is None or len(matched.group(1)) == 0:
            plpy.error("Elastic Net error: Independent variable format is not quite right!")
        outstr_array = _string_to_array(matched.group(1))
    except:
        plpy.error("Elastic Net error: Independent variable format is not quite right!")
    return (col_ind_var, outstr_array)

# ========================================================================

def elastic_net_predict_all (schema_madlib, tbl_model, tbl_new_source,
                             col_id, tbl_predict, **kwargs):
    """
    Predict and put the result in a table. Useful for general CV
    """
    regress_family = plpy.execute(
        """
        select family from {tbl_model}
        """.format(tbl_model = tbl_model))[0]["family"]

    if regress_family.lower() == "gaussian" or regress_family.lower() == "linear":
        return __elastic_net_gaussian_predict_all(schema_madlib, tbl_model, tbl_new_source,
                                                  tbl_predict, col_id)

    if regress_family.lower() == "binomial" or regress_family.lower() == "logistic":
        return __elastic_net_binomial_predict_all(schema_madlib, tbl_model, tbl_new_source,
                                                  tbl_predict, col_id)

    plpy.error("Not a supported response family!")
    return None

# ========================================================================

def __elastic_net_gaussian_predict_all(schema_madlib, tbl_model, tbl_new_source,
                                       tbl_predict, col_id):
    """
    Prediction for Gaussian model
    """
    old_msg_level = plpy.execute(
        """
        select setting from pg_settings
        where name='client_min_messages'
        """)[0]['setting']
    plpy.execute("set client_min_messages to error")
    
    if col_id is None:
        col_id_str = "NULL"
    else:
        col_id_str = col_id
 
    dense_vars = mad_vec(plpy.execute(
        """
        select features_selected as fs
        from {tbl_model}
        """.format(tbl_model = tbl_model))[0]["fs"])

    dense_vars_str = "array["
    for i in range(len(dense_vars)):
        dense_vars_str += str(dense_vars[i])
        if i != len(dense_vars) - 1:
            dense_vars_str += ", "
    dense_vars_str += "]"
  
    # Must be careful to avoid possible name conflicts
    plpy.execute(
        """
        drop table if exists {tbl_predict};
        create table {tbl_predict} as
            select
                id,
                {schema_madlib}.elastic_net_gaussian_predict(coef_nonzero,
                                                            intercept, ind_var)
                                 as prediction
            from
                {tbl_model} as tbl1,
                (select
                    {col_id} as id,
                    {dense_vars_str} as ind_var
                from
                    {tbl_new_source}) tbl2
        """.format(tbl_model = tbl_model,
                   tbl_predict = tbl_predict,
                   col_id = col_id_str,
                   dense_vars_str = dense_vars_str,
                   schema_madlib = schema_madlib,
                   tbl_new_source = tbl_new_source))
 
    plpy.execute("set client_min_messages to " + old_msg_level)
    return None

# ========================================================================

def __elastic_net_binomial_predict_all(schema_madlib, tbl_model, tbl_new_source,
                                       tbl_predict, col_id):
    """
    Prediction for Binomial model
    """
    old_msg_level = plpy.execute(
        """
        select setting from pg_settings
        where name='client_min_messages'
        """)[0]['setting']
    plpy.execute("set client_min_messages to error")
    
    if col_id is None:
        col_id_str = "NULL"
    else:
        col_id_str = col_id
 
    dense_vars = mad_vec(plpy.execute(
        """
        select features_selected as fs
        from {tbl_model}
        """.format(tbl_model = tbl_model))[0]["fs"])

    dense_vars_str = "array["
    for i in range(len(dense_vars)):
        dense_vars_str += str(dense_vars[i])
        if i != len(dense_vars) - 1:
            dense_vars_str += ", "
    dense_vars_str += "]"
  
    # Must be careful to avoid possible name conflicts
    plpy.execute(
        """
        drop table if exists {tbl_predict};
        create table {tbl_predict} as
            select
                id,
                {schema_madlib}.elastic_net_binomial_predict(coef_nonzero,
                                                            intercept, ind_var)
                                 as prediction
            from
                {tbl_model} as tbl1,
                (select
                    {col_id} as id,
                    {dense_vars_str} as ind_var
                from
                    {tbl_new_source}) tbl2
        """.format(tbl_model = tbl_model,
                   tbl_predict = tbl_predict,
                   col_id = col_id_str,
                   dense_vars_str = dense_vars_str,
                   schema_madlib = schema_madlib,
                   tbl_new_source = tbl_new_source))
 
    plpy.execute("set client_min_messages to " + old_msg_level)
    return None
