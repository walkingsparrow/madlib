
import plpy
import re
from elastic_net_gaussian_igd import __elastic_net_gaussian_igd_train
from elastic_net_gaussian_fista import __elastic_net_gaussian_fista_train
from utilities.validate_args import __is_col_exists
from utilities.utilities import _string_to_array

# ========================================================================

def elastic_net_help(schema_madlib, family_or_optimizer, **kwargs):
    """
    Given a response family name or optimizer name, print out the related
    information.

    If a family name is given, print out the supported optimizer together
    with its default optimizer.
    
    If an optimizer name is given, print out the necessary parameters.
    """
    if (family_or_optimizer.lower() == "gaussian" or
        family_or_optimizer == "linear"):
        return """
        Supported optimizer:
        (1) Incremental gradient descent method ('igd')
        (2) Fast iterative shrinkage thesholding algorithm ('fista')  

        For alpha = 0, Newton method ('newton') is also supported.

        Run:
        SELECT {schema_madlib}.elastic_net_train('optimizer');
        to see more help on each optimizer.
        """.format(schema_madlib = schema_madlib)

    if family_or_optimizer.lower() == "igd":
        return """
        Incremental gradient descent (IGD) method

        In order to obtain sparse coefficients, a
        modified version of IGD is actually used.

        Parameters:
        stepsize         - default 0.01
        threshold        - default 1e-10. When a coefficient is really
                           small, set it to be 0
        warmup           - default is False
        warmup_lambdas   - default in Null
        warmup_lambda_no - default 15. How many lambda's are used in
                           warm-up, will be overridden if warmup_lambdas   
                           is not NULL

        When warmup is True and warmup_lambdas is NULL, a series
        of lambda values will be automatically generated and used.

        Reference:
        [1] Shai Shalev-Shwartz and Ambuj Tewari, Stochastic Methods for l1    
            Regularized Loss Minimization. Proceedings of the 26th Interna-   
            tional Conference on Machine Learning, Montreal, Canada, 2009.   
        """

    if family_or_optimizer.lower() == "fista":
        return """
        Fast iterative shrinkage thesholding algorithm
        with backtracking for stepsizes

        Parameters:
        max_stepsize     - default is 4.0
        eta              - default 2, if stepsize does not work
                           stepsize/eta will be tried
        warmup           - default is False
        warmup_lambdas   - default in Null
        warmup_lambda_no - default 15. How many lambda's are used in
                           warm-up, will be overridden if warmup_lambdas     
                           is not NULL

        When warmup is True and warmup_lambdas is NULL, a series
        of lambda values will be automatically generated and used.
                
        Reference:
        [1] Beck, A. and M. Teboulle (2009), A fast iterative   
            shrinkage-thresholding algorothm for linear inverse   
            problems. SIAM J. on Imaging Sciences 2(1), 183-202.   
        """
        
    if family_or_optimizer.lower() == "newton":
        return "Newton method  "
        
    return """
    Not a supported response family or optimizer  
    """
        

# ========================================================================

# lambda is a keyword of Python, try to avoid using
# it, so use lambda_value instead
def elastic_net_train(schema_madlib, tbl_source, tbl_result, col_dep_var,
                      col_ind_var, regress_family, alpha, lambda_value,
                      standardize, grouping_col, optimizer,
                      optimizer_params, excluded, max_iter, tolerance,
                      **kwargs):
    """
    A wrapper for all variants of elastic net regularization.

    @param tbl_source        Name of data source table
    @param col_ind_var       Name of independent variable column,
                             independent variable is an array
    @param col_dep_var       Name of dependent variable column
    @param tbl_result        Name of the table to store the results,
                             will return fitting coefficients and
                             likelihood
    @param lambda_value      The regularization parameter
    @param alpha             The elastic net parameter, [0, 1]
    @param standardize       Whether to normalize the variables
    @param regress_family    Response type, 'gaussian' or 'binomial'
    @param optimizer         The optimization algorithm, for example 'igd'
    @param optimizer_params  Parameters of the above optimizer, the format
                             is '{arg = value, ...}'::varchar[]
    @param excluded          Which variables are excluded when
                             col_ind_var == "*"
    """
    # handle all special cases of col_ind_var
    (col_ind_var, outstr_array) = analyze_input_str(schema_madlib, tbl_source, col_ind_var, 
                                                    col_dep_var, excluded)
    # Special case for ridge linear regression
    if ((regress_family.lower() == "gaussian" or regress_family.lower() == "linear") and
        optimizer.lower() == "newton" and
        alpha == 0):
        plpy.execute("""select {schema_madlib}.ridge_newton_train(
                            '{tbl_source}', '{col_ind_var}', '{col_dep_var}',
                            '{tbl_result}', {lambda_value}, {standardize}
        )""".format(schema_madlib = schema_madlib,
                    tbl_source = tbl_source,
                    col_ind_var = col_ind_var,
                    col_dep_var = col_dep_var,
                    tbl_result = tbl_result,
                    lambda_value = lambda_value,
                    standardize = standardize))
        return None
    
    if ((regress_family.lower() == "gaussian" or regress_family.lower() == "linear") and
        optimizer.lower() == "igd"):
        __elastic_net_gaussian_igd_train(schema_madlib, tbl_source, col_ind_var,
                                         col_dep_var, tbl_result, lambda_value, alpha,
                                         standardize, optimizer_params, max_iter,
                                         tolerance, outstr_array, **kwargs)
        return None

    if ((regress_family.lower() == "gaussian" or regress_family.lower() == "linear") and
        optimizer.lower() == "fista"):
        __elastic_net_gaussian_fista_train(schema_madlib, tbl_source, col_ind_var,
                                           col_dep_var, tbl_result, lambda_value, alpha,
                                           standardize, optimizer_params, max_iter,
                                           tolerance, outstr_array, **kwargs)
        return None

    plpy.error("Not a supported response family or supported optimizer of the given response family!")
    return None

# ========================================================================

def analyze_input_str(schema_madlib, tbl_source,
                      col_ind_var, col_dep_var, excluded):
    """
    Make input strings and output strings compatible with functions

    @param tbl_source Data table
    @param col_ind_var Independent variables
    @param col_dep_var Dependent variables
    @param excluded Which variables are excluded when col_ind_var == "*"
    """
    outstr_array = []

    dimension = plpy.execute(
        """
        select array_upper({col_ind_var},1) as dimension
        from {tbl_source} limit 1
        """.format(tbl_source = tbl_source,
                   col_ind_var = col_ind_var))[0]["dimension"]
    
    if col_ind_var == "*":
        cols = __get_cols(tbl_source, schema_madlib)
        if excluded is not None:
            s = _string_to_array(excluded)
        else:
            s = []

        col_ind_var_new = "array["
        
        for i in range(len(cols)):
            if cols[i] not in s and cols[i] != col_dep_var:
                col_ind_var_new += cols[i]
                outstr_array.append(cols[i])
                if i != len(cols) - 1:
                    col_ind_var_new += ", "
                else:
                    col_ind_var_new += "]"
        return (col_ind_var_new, outstr_array)

    elif __is_col_exists(tbl_source, [col_ind_var], schema_madlib):
        # a single column is independent variable
        # which means that it is an array
        # excluded must be a string contains integers
        if excluded is not None:
            s = _string_to_array(excluded)
            try:
                for i in range(len(s)):
                    s[i] = int(s[i])
            except:
                plpy.error("Elastic Net error: When the independent variable is an array, excluded can only be integers!")
        else:
            s = []

        if excluded is None:
            col_ind_var_new = col_ind_var               
            outstr_array = map(lambda x: str(x + 1), range(dimension))
        else:
            col_ind_var_new = "array["
            for i in range(dimension):
                if i+1 not in s:
                    col_ind_var_new += col_ind_var + "[" + str(i+1) + "]"
                    outstr_array.append(str(i+1))
                    if i != dimension - 1:
                        col_ind_var_new += ", "
                    else:
                        col_ind_var_new += "]"
                        
        return (col_ind_var_new, outstr_array)
            
    try:
        matched = re.match(r"(?i)^array\[(.*)\]$", col_ind_var)
        if matched is None:
            plpy.error("Elastic Net error: Independent variable format is not quite right!")
        outstr_array = _string_to_array(matched.group(1))
    except:
        plpy.error("Elastic Net error: Independent variable format is not quite right!")
    return (col_ind_var, outstr_array)

# ========================================================================

def elastic_net_predict(schema_madlib, regress_family, coefficients,
                        intercept, ind_var, **kwargs):
    """
    Make predictions using the fitting coefficients
    """
    if regress_family.lower() == "gaussian" or regress_family.lower() == "linear":
        return __elastic_net_gaussian_predict(coefficients, intercept, ind_var)

    plpy.error("Not a supported response family!")
    return None

# ========================================================================
    
def __elastic_net_gaussian_predict(coefficients, intercept, ind_var):
    """
    Prediction for linear models
    """
    dot = intercept
    for i in range(len(coefficients)):
        dot += coefficients[i] * ind_var[i]
    return dot

    