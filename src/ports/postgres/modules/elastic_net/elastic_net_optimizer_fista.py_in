
## Try to make every function has a useful return value !
## Try to avoid any changes to function arguments !

import plpy
import math
from elastic_net_utils import __normalize_data
from elastic_net_utils import __compute_data_scales
from elastic_net_utils import __compute_means
from elastic_net_utils import __tbl_dimension_rownum
from utilities.utilities import __unique_string
from utilities.control import IterationController
from elastic_net_utils import IterationControllerNoTableDrop
from elastic_net_utils import __elastic_net_validate_args
from elastic_net_utils import __compute_log_likelihood
from utilities.utilities import _array_to_string
from elastic_net_utils import __compute_average_sq
from elastic_net_utils import __generate_warmup_lambda_sequence
from elastic_net_utils import __process_warmup_lambdas
from elastic_net_generate_result import __elastic_net_generate_result
from utilities.utilities import __mad_version
from elastic_net_utils import __preprocess_optimizer_params

version_wrapper = __mad_version()
mad_vec = version_wrapper.select_vecfunc()

## ========================================================================

def __fista_params_parser(optimizer_params, lambda_value, tolerance, schema_madlib):
    """
    Parse fista parameters.
    """
    allowed_params = set(["max_stepsize", "eta", "warmup", "warmup_lambdas",
                          "warmup_lambda_no", "use_active_set", "random_stepsize",
                          "activeset_tolerance", "warmup_tolerance"])
    name_value = dict()
    # default values
    name_value["max_stepsize"] = 2.
    name_value["use_active_set"] = 0 # use of active set
    name_value["eta"] = 1.2
    name_value["warmup"] = False
    name_value["warmup_lambdas"] = None
    name_value["warmup_lambda_no"] = 15
    name_value["random_stepsize"] = 0 # use random stepsize
    name_value["activeset_tolerance"] = tolerance
    name_value["warmup_tolerance"] = tolerance

    warmup_lambdas = None
    warmup_lambda_no = None

    if optimizer_params is None or len(optimizer_params) == 0:
        return name_value
    
    for s in __preprocess_optimizer_params(optimizer_params):
        items = s.split("=")
        if (len(items) != 2):
            plpy.error("Elastic Net error: Optimizer parameter list has incorrect format!")
        param_name = items[0].strip(" \"").lower()
        param_value = items[1].strip(" \"").lower()

        if param_name not in allowed_params:
            plpy.error(
                """
                Elastic Net error: {param_name} is not a valid parameter name for the FISTA optimizer.
                Run:
                
                SELECT {schema_madlib}.elastic_net_train('fista');

                to see the parameters for FISTA algorithm.
                """.format(param_name = param_name,
                           schema_madlib = schema_madlib))

        if param_name == "activeset_tolerance":
            try:
                name_value["activeset_tolerance"] = float(param_value)
            except:
                plpy.error("Elastic Net error: activeset_tolerance must be a float number!")

        if param_name == "warmup_tolerance":
            try:
                name_value["warmup_tolerance"] = float(param_value)
            except:
                plpy.error("Elastic Net error: warmup_tolerance must be a float number!")

        if param_name == "max_stepsize":
            try:
                name_value["max_stepsize"] = float(param_value)
            except:
                plpy.error("Elastic Net error: max_stepsize must be a float number!")

        if param_name == "eta":
            try:
                name_value["eta"] = float(param_value)
            except:
                plpy.error("Elastic Net error: eta must be a float number!")

        if param_name == "random_stepsize":
            if param_value in ["true", "t", "yes", "y"]:
                name_value["random_stepsize"] = 1
            elif param_value in ["false", "f", "no", "n"]:
                name_value["random_stepsize"] = 0
            else:
                plpy.error("Elastic Net error: Do you need to add some randomness to step size (True/False or yes/no) ?")

        if param_name == "warmup":
            if param_value in ["true", "t", "yes", "y"]:
                name_value["warmup"] = True
            elif param_value in ["false", "f", "no", "n"]:
                name_value["warmup"] = False
            else:
                plpy.error("Elastic Net error: Do you need warmup (True/False or yes/no) ?")

        if param_name == "warmup_lambdas" and param_value != "null":
            warmup_lambdas = param_value

        if param_name == "warmup_lambda_no":
            warmup_lambda_no = param_value

        if param_name == "use_active_set":
            if param_value in ["true", "t", "yes", "y"]:
                name_value["use_active_set"] = 1
            elif param_value in ["false", "f", "no", "n"]:
                name_value["use_active_set"] = 0
            else:
                plpy.error("Elastic Net error: Do you need warmup (True/False or yes/no) ?")

    if name_value["warmup"]:
        if warmup_lambdas is not None:
            # errors are handled in __process_warmup_lambdas
            name_value["warmup_lambdas"] = __process_warmup_lambdas(warmup_lambdas, lambda_value)
        if warmup_lambda_no is not None:
            try:
                name_value["warmup_lambda_no"] = int(warmup_lambda_no)
            except:
                plpy.error("Elastic Net error: warmup_lambda_no must be an integer!")
            
    # validate the parameters
    if name_value["max_stepsize"] <= 0:
        plpy.error("Elastic Net error: backtracking parameter max_stepsize must be positive!")

    if name_value["eta"] < 1:
        plpy.error("Elastic Net error: backtracking parameter eta must be larger than 1!")

    if (name_value["warmup"] and name_value["warmup_lambdas"] is None and
        name_value["warmup_lambda_no"] < 1):
        plpy.error("Elastic Net error: Number of warm-up lambdas must be a positive integer!")

    if name_value["activeset_tolerance"] <= 0:
        plpy.error("Elastic Net error: activeset_tolerance must be positive!")

    if name_value["warmup_tolerance"] <= 0:
        plpy.error("Elastic Net error: warmup_tolerance must be positive!")

    return name_value
    
## ========================================================================

def __fista_create_tbl_args(**args):
    """
    create the temporary schema and argument table used in FISTA iterations
    """
    plpy.execute("""
                 drop table if exists {tbl_fista_args};       
                 create temp table {tbl_fista_args} (
                     {dimension_name}       integer,
                     {lambda_name}          double precision[],
                     {alpha_name}           double precision,
                     {total_rows_name}      integer,
                     {max_iter_name}        integer,
                     {tolerance_name}       double precision,
                     {max_stepsize_name}    double precision,
                     {eta_name}             double precision,
                     {activeset_name}       integer
                 );
                 """.format(**args))
    plpy.execute("""
                 insert into {tbl_fista_args} values
                     ({dimension}, '{warmup_lambdas}'::double precision[],
                     {alpha}, {row_num}, {max_iter}, {tolerance},
                     {max_stepsize}, {eta},
                     {use_active_set});
                 """.format(**args))

    return None
    
## ========================================================================

def __fista_construct_dict(schema_madlib, family, tbl_source, col_ind_var, col_dep_var,
                         tbl_result, dimension, row_num, lambda_value, alpha,
                         normalization, max_iter, tolerance, outstr_array, optimizer_params_dict):
    """
    Construct the dict used by a series of SQL queries in FISTA optimizer.
    """
    args = dict(schema_madlib = schema_madlib,
                family = family,
                tbl_source = tbl_source,
                tbl_data = tbl_source, # argument name used in normalization
                col_ind_var = col_ind_var, col_dep_var = col_dep_var,
                col_ind_var_norm_new = __unique_string(), # for normalization usage
                col_ind_var_tmp = __unique_string(),
                col_dep_var_norm_new = __unique_string(), # for normalization usage
                col_dep_var_tmp = __unique_string(),
                tbl_result = tbl_result,
                lambda_value = lambda_value, alpha = alpha,
                dimension = dimension, row_num = row_num,
                max_iter = max_iter, tolerance = tolerance,
                outstr_array = outstr_array,
                normalization = normalization)
    
    # Add the optimizer parameters
    args.update(optimizer_params_dict)

    # Table names useful when normalizing the original data
    # Note: in order to be consistent with the calling convention
    # of the normalization functions, multiple elements of the dict
    # actually have the same value. This is a price that one has to pay
    # if he wants to save typing argument names by using **args as the
    # function argument.
    tbl_ind_scales = __unique_string()
    tbl_dep_scale = __unique_string()
    tbl_data_scaled = __unique_string()
    args.update(tbl_scale = tbl_dep_scale, tbl_dep_scale = tbl_dep_scale,
                tbl_scales = tbl_ind_scales, tbl_ind_scales = tbl_ind_scales,
                tbl_data_scaled = tbl_data_scaled)

    # Table names used in IGD iterations
    args.update(tbl_fista_state = __unique_string(),
                tbl_fista_args = __unique_string())

    # more, for args table
    args["dimension_name"] = __unique_string()
    args["lambda_name"] = __unique_string()
    args["alpha_name"] = __unique_string()
    args["total_rows_name"] = __unique_string()
    args["max_iter_name"] = __unique_string()
    args["tolerance_name"] = __unique_string()
    args["max_stepsize_name"] = __unique_string()
    args["eta_name"] = __unique_string()
    args["activeset_name"] = __unique_string()
   
    return args

## ========================================================================

def __fista_cleanup_temp_tbls(**args):
    """
    Drop all temporary tables used by FISTA optimizer,
    including tables used in the possible normalization
    and FISTA iterations.
    """
    plpy.execute("""
                 drop table if exists {tbl_ind_scales};
                 drop table if exists {tbl_dep_scale};
                 drop table if exists {tbl_data_scaled};
                 drop table if exists {tbl_fista_args};
                 drop table if exists pg_temp.{tbl_fista_state};
                 """.format(**args))
  
    return None

## ========================================================================

def __elastic_net_fista_train(schema_madlib, func_step_aggregate,
                              func_state_diff, family,
                              tbl_source, col_ind_var,
                              col_dep_var, tbl_result, lambda_value, alpha,
                              normalization, optimizer_params, max_iter,
                              tolerance, outstr_array, **kwargs):
    """
    func_step_aggregate is string, and it is the name of the step function
    """
    __elastic_net_validate_args(tbl_source, col_ind_var, col_dep_var,
                                tbl_result, lambda_value, alpha,
                                normalization, max_iter, tolerance)
    
    return __elastic_net_fista_train_compute(schema_madlib,
                                             func_step_aggregate,
                                             func_state_diff,
                                             family,
                                             tbl_source, col_ind_var,
                                             col_dep_var, tbl_result,
                                             lambda_value, alpha,
                                             normalization,
                                             optimizer_params, max_iter,
                                             tolerance, outstr_array,
                                             **kwargs)

## ========================================================================
    
def __elastic_net_fista_train_compute (schema_madlib, func_step_aggregate,
                                       func_state_diff, family,
                                       tbl_source, col_ind_var,
                                       col_dep_var, tbl_result, lambda_value, alpha,
                                       normalization, optimizer_params, max_iter,
                                       tolerance, outstr_array, **kwargs):
    """
    Fit linear model with elastic net regularization using FISTA optimization.

    @param tbl_source        Name of data source table
    @param col_ind_var       Name of independent variable column,
                             independent variable is an array
    @param col_dep_var       Name of dependent variable column
    @param tbl_result        Name of the table to store the results,
                             will return fitting coefficients and
                             likelihood
    @param lambda_value      The regularization parameter
    @param alpha             The elastic net parameter, [0, 1]
    @param normalization     Whether to normalize the variables
    @param optimizer_params  Parameters of the above optimizer, the format
                             is '{arg = value, ...}'::varchar[]
    """
    old_msg_level = plpy.execute("""
                                 select setting from pg_settings
                                 where name='client_min_messages'
                                 """)[0]['setting']
    plpy.execute("set client_min_messages to error")

    (dimension, row_num) = __tbl_dimension_rownum(tbl_source, col_ind_var)

    # generate a full dict to ease the following string format
    # including several temporary table names
    args = __fista_construct_dict(schema_madlib, family, tbl_source, col_ind_var,
                                  col_dep_var, tbl_result,
                                  dimension, row_num, lambda_value,
                                  alpha, normalization,
                                  max_iter, tolerance, outstr_array,
                                  __fista_params_parser(optimizer_params,
                                                        lambda_value,
                                                        tolerance,
                                                        schema_madlib))

    # use normalized data or not
    if normalization:
        __normalize_data(args)
        tbl_used = args["tbl_data_scaled"]
        args["col_ind_var_new"] = args["col_ind_var_norm_new"]
        args["col_dep_var_new"] = args["col_dep_var_norm_new"]
    else:
        tbl_used = tbl_source
        args["col_ind_var_new"] = col_ind_var
        args["col_dep_var_new"] = col_dep_var

    args["tbl_used"] = tbl_used

    if args["warmup_lambdas"] is not None:
        args["warm_no"] = len(args["warmup_lambdas"])
        args["warmup_lambdas"] = _array_to_string(args["warmup_lambdas"])
    
    if args["warmup"] and args["warmup_lambdas"] is None:
        # average squares of each feature
        # used to estimate the largest lambda value
        args["sq"] = __compute_average_sq(**args)
        args["warmup_lambdas"] = __generate_warmup_lambda_sequence(tbl_used, args["col_ind_var_new"],
                                                                   args["col_dep_var_new"],
                                                                   dimension, row_num, lambda_value, alpha,
                                                                   args["warmup_lambda_no"], args["sq"])
        args["warm_no"] = len(args["warmup_lambdas"])
        args["warmup_lambdas"] = _array_to_string(args["warmup_lambdas"])
    elif args["warmup"] is False:
        args["warm_no"] = 1
        args["warmup_lambdas"] = _array_to_string([lambda_value]) # only one value

    # create the temp table that passes parameter values to FISTA optimizer
    __fista_create_tbl_args(**args)
        
    # perform the actual calculation
    iteration_run = __compute_fista(schema_madlib, func_step_aggregate,
                                    func_state_diff,
                                    args["tbl_fista_args"],
                                    args["tbl_fista_state"], tbl_used,
                                    args["col_ind_var_new"],
                                    args["col_dep_var_new"],
                                    start_iter = 0,
                                    tolerance = args["tolerance"],
                                    activeset_tolerance = args["activeset_tolerance"],
                                    warmup_tolerance = args["warmup_tolerance"],
                                    max_iter = args["max_iter"],
                                    warm_no = args["warm_no"],
                                    random_stepsize = args["random_stepsize"],
                                    use_active_set = args["use_active_set"],
                                    dimension_name = args["dimension_name"],
                                    lambda_name = args["lambda_name"],
                                    alpha_name = args["alpha_name"],
                                    total_rows_name = args["total_rows_name"],
                                    max_iter_name = args["max_iter_name"],
                                    max_stepsize_name = args["max_stepsize_name"],
                                    eta_name = args["eta_name"],
                                    activeset_name = args["activeset_name"],
                                    tolerance_name = args["tolerance_name"])

    __elastic_net_generate_result("fista", iteration_run, **args)
    
    # cleanup    
    __fista_cleanup_temp_tbls(**args)
    plpy.execute("set client_min_messages to " + old_msg_level)
    return None
    
## ========================================================================

def __compute_fista(schema_madlib, func_step_aggregate, func_state_diff,
                    tbl_args, tbl_state, tbl_source,
                    col_ind_var, col_dep_var, start_iter, **kwargs):
    """
    Driver function for elastic net using FISTA

    @param schema_madlib Name of the MADlib schema, properly escaped/quoted
    @param tbl_args Name of the (temporary) table containing all non-template
        arguments
    @param tbl_state Name of the (temporary) table containing the inter-iteration
        states
    @param rel_source Name of the relation containing input points
    @param col_ind_var Name of the independent variables column
    @param col_dep_var Name of the dependent variable column
    @param drop_table Boolean, whether to use IterationController (True) or
                      IterationControllerNoTableDrop (False)
    @param kwargs We allow the caller to specify additional arguments (all of
        which will be ignored though). The purpose of this is to allow the
        caller to unpack a dictionary whose element set is a superset of
        the required arguments by this function.
    
    @return The iteration number (i.e., the key) with which to look up the
        result in \c tbl_state
    """
    iterationCtrl = IterationController( #TableAppend(
        func_step_aggregate = func_step_aggregate,
        func_state_diff = func_state_diff,
        rel_args = tbl_args,
        rel_state = tbl_state,
        stateType = "double precision[]",
        truncAfterIteration = False,
        schema_madlib = schema_madlib, # Identifiers start here
        rel_source = tbl_source,
        col_ind_var = col_ind_var,
        col_dep_var = col_dep_var,
        lambda_count = 1,
        is_active = 0,
        **kwargs)

    state_size = None
    
    with iterationCtrl as it:
        it.iteration = start_iter
        while True:
            # manually add the intercept term
            it.update("""
                      select
                          {schema_madlib}.{func_step_aggregate}(
                              ({col_ind_var})::double precision[],
                              ({col_dep_var}),
                              (select _state from {rel_state}
                                  where _iteration = {iteration}),
                              (_args.{lambda_name}[{lambda_count}])::double precision,
                              (_args.{alpha_name})::double precision,
                              (_args.{dimension_name})::integer,
                              (_args.{total_rows_name})::integer,
                              (_args.{max_stepsize_name})::double precision,
                              (_args.{eta_name})::double precision,
                              (_args.{activeset_name})::integer,
                              {is_active}::integer,
                              {random_stepsize}::integer
                          )
                      from {rel_source} as _src, {rel_args} as _args
                      """)

            if it.kwargs["is_active"] == 1:
                it.kwargs["use_tolerance"] = it.kwargs["activeset_tolerance"]
            elif it.kwargs["lambda_count"] < it.kwargs["warm_no"]:
                it.kwargs["use_tolerance"] = it.kwargs["warmup_tolerance"]
            else:
                it.kwargs["use_tolerance"] = it.kwargs["tolerance"]

            if it.kwargs["use_active_set"] == 1:
                if state_size is None:
                    state_size = plpy.execute(
                        """
                        select array_upper(_state, 1) as size
                        from {rel_state} limit 1;
                        """.format(**it.kwargs))[0]["size"]

                is_backtracking = plpy.execute(
                    """
                    select _state[{state_size}] as backtracking
                    from {rel_state}
                    where _iteration = {iteration}
                    """.format(state_size = state_size,
                               iteration = it.iteration,
                               **it.kwargs))[0]["backtracking"]
                
                if it.test(
                    """
                    {iteration} >= _args.{max_iter_name}
                    or
                    {schema_madlib}.{func_state_diff}(
                        (select _state from {rel_state}
                            where _iteration = {iteration} - 1),
                        (select _state from {rel_state}
                            where _iteration = {iteration})) < {use_tolerance}
                    """):
                    if it.iteration < it.kwargs["max_iter"]:
                        if it.kwargs["is_active"] == 0:
                            if (it.kwargs["lambda_count"] < it.kwargs["warm_no"]):
                                it.kwargs["lambda_count"] += 1
                            else:
                                break
                        else:
                            it.kwargs["is_active"] = 0
                    else:
                        break
                else:
                    # change active state only outside of backtracking
                    if is_backtracking == 0 and it.kwargs["is_active"] == 0:
                        it.kwargs["is_active"] = 1
            else:
                if it.test(
                    """
                    {iteration} >= _args.{max_iter_name} or
                    {schema_madlib}.{func_state_diff}(
                        (select _state from {rel_state}
                            where _iteration = {iteration} - 1),
                        (select _state from {rel_state}
                            where _iteration = {iteration})) < _args.{tolerance_name}
                    """):
                    if (it.iteration < it.kwargs["max_iter"] and
                        it.kwargs["lambda_count"] < it.kwargs["warm_no"]):
                        it.kwargs["lambda_count"] += 1
                    else:
                        break
                        
        if it.kwargs["lambda_count"] < it.kwargs["warm_no"]:
            plpy.error("""
                       Elastic Net error: The final target lambda value is not
                       reached in warm-up iterations. You need more iterations!
                       """)
            
    return iterationCtrl.iteration
