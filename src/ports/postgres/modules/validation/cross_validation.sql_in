
/* ----------------------------------------------------------------------- *//** 
 *
 * @file cross_validation.sql_in
 *
 * @brief SQL functions for cross validation
 * @date January 2011
 *
 * @sa For a brief introduction to the usage of cross validation, see the
 *     module description \ref grp_validation.
 *
 *//* ----------------------------------------------------------------------- */


m4_include(`SQLCommon.m4') --'

/**
@addtogroup grp_validation

@about

Cross-validation, sometimes called rotation estimation, is a technique for assessing how the results of a statistical
analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and
one wants to estimate how accurately a predictive model will perform in practice. One round of cross-validation
involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called
the training set), and validating the analysis on the other subset (called the validation set or testing set). To
reduce variability, multiple rounds of cross-validation are performed using different partitions, and the validation
results are averaged over the rounds.


In k-fold cross-validation, the original sample is randomly partitioned into k equal size subsamples. Of the k subsamples,
a single subsample is retained as the validation data for testing the model, and the remaining k âˆ’ 1 subsamples are used
as training data. The cross-validation process is then repeated k times (the folds), with each of the k subsamples used
exactly once as the validation data. The k results from the folds then can be averaged (or otherwise combined) to produce
a single estimation. The advantage of this method over repeated random sub-sampling is that all observations are used for
both training and validation, and each observation is used for validation exactly once. 10-fold cross-validation is
commonly used, but in general k remains an unfixed parameter. 

@input

The input includes the data set, a training function, a prediction function and an error metric function.


The training function takes in a given data set with independent and dependent variables in it and produces
a model, which is stored in an output table.


The prediction function takes in the model generated by the training function and a different data set with
only independent variables in it, and it produces a prediction of the dependent variables bease on the model.


The error metric function takes in the prediction made by the prediction function, and compare with the known
values of the dependent variables of the data set that was fed into the prediction function. It computes the
error metric defined by the function.


Other inputs include the output table name, k value for the k-fold cross-validation, and how many folds the user
wants to try (for example, the user can choose to run a simple validation instead of a full cross-validation.)

@usage

In order to choose the optimum value for a parameter of the model, the user needs to provied the training function,
prediction function, error metric function, the parameter and its values to be studied and the data set.

It would be better if the data set has a unique ID for each row, so that it is easier to cut the data set into the
training part and the validation part. The user also needs to inform the cross validation (CV) function about whether this
ID value is randomly assigned to each row. If it is not randomly assigned, the CV function will automatically generate
a random ID for each row.

If the data set has no unique ID for each row, the CV function will copy the data set and create a randomly assigned ID
column for the newly created temp table. The new table will be dropped after the computation is finished. To minimize
the copying work load, the user needs to provide the data column names (for independent variables and dependent
variables) that are going to be used in the calculation, and only these columns will be copied.

<pre>SELECT cross_validation_general(
    <em>modelling_func</em>,              -- Name of function that trains the model
    <em>modelling_params</em>,         -- Array of parameters for modelling function
    <em>modelling_params_type</em>, -- Types of each parameters for modelling function
    --
    <em>param_explored</em>,        -- Name of parameter that will be checked to find the optimum value, the
                                    ---- same name must also appear in the array of modelling_params
    <em>explore_values</em>,        -- Values of this parameter that will be studied
    --
    <em>predict_func</em>,          -- Name of function for prediction
    <em>predict_params</em>,        -- Array of parameters for prediction function
    <em>predict_params_type</em>,   -- Types of each parameters for prediction function
    --
    <em>metric_func</em>,           -- Name of function for measuring errors
    <em>metric_params</em>,         -- Array of parameters for error metric function
    <em>metric_params_type</em>,    -- Types of each parameters for metric function
    --
    <em>data_tbl</em>,              -- Data table which will be split into training and validation parts
    <em>data_id</em>,               -- Name of the unique ID associated with each row. Provide <em>NULL</em>
                                    ---- if there is no such column in the data table
    <em>id_is_random</em>,          -- Whether the provided ID is randomly assigned to each row
    --
    <em>validation_result</em>,     -- Table name to store the output of CV function, see the Output for
                                    ---- format. It will be automatically created by CV function
    --
    <em>fold_num</em>,              -- Value of k. How many folds validation? Each validation uses 1/fold_num
                                    ---- fraction of the data for validation. This value can be a non-integer,
                                    ---- in which case usually one validation is proper (see the comment to
                                    ---- <em>upto_fold</em>). Deafult value: 10.
    <em>upto_fold</em>,             -- How many fold validation actually will be calculated? Default
                                    ---- value: fold_num::INTEGER. If it is 1, then CV function just runs
                                    ---- one validation for each value of <em>param_explored</em>
    <em>data_cols</em>              -- Names of data columns that are going to be used. It is only useful when
                                    ---- <em>data_id</em> is NULL, otherwise it is ignored.
);</pre>

Special keywords in parameter arrays of modelling, prediction and metric functions:

<em>\%data%</em> : The argument position for training/validation data 

<em>\%model%</em> : The argument position for the output/input of modelling/prediction function

<em>\%id%</em> : The argument position of unique ID column (provided by user or generated by CV function as is mentioned above)

<em>\%prediction%</em> : The argument position for the output/input of prediction/metric function

<em>\%error%</em> : The argument position for the output of metric function

Output:
<pre>  param_explored | average error | standard deviation of error
-------------------------|------------------|--------------------------------
                    .......
</pre>

Note:

<em>max_locks_per_transaction</em>, which usually has the default value of 64, limits the number of tables that can be
dropped inside a single transaction (the CV function). Thus the number of different values of <em>param_explored</em>
(or the length of array <em>explored_values</em>) cannot be too large. For 10-fold cross validation, the limit of
length(<em>explored_values</em>) is around 40. If this number is too large, the use might see "out of shared memory"
error because <em>max_locks_per_transaction</em> is used up.

One way to overcome this limitation is to run CV function multiple times, and each run covers a different region of
values of the parameter.

If the user wants to study more values of the parameter in a single run of the CV function, cross validation functions
for specific modules can be used instead. For example, the ridge regression module provides its own CV function named
<em>\ref ridge_newton_cv</em>, which does not have such a limitation.

@examp

Cross validation is used on ridge regreesion to find the best value of the regulation parameter.

(1) Populate the table 'cvtest' with 101 dimensional independent variable 'val', and dependent
variable 'dep'.

(2) Run the general CV function
<pre>
select madlib.cross_validation_general(
    'madlib.ridge_newton_train',
    '{\%data%, val, dep, \%model%, lambda, FALSE, 101}'::varchar[],
    '{varchar, varchar, varchar, varchar, double precision, boolean, integer}'::varchar[],
    --
    'lambda',
    '{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41}'::varchar[],
    --
    'madlib.ridge_newton_predict',
    '{\%model%, \%data%, val, \%id%, \%prediction%}'::varchar[],
    '{varchar, varchar, varchar, varchar, varchar}'::varchar[],
    --
    'madlib.ridge_newton_error',
    '{\%prediction%, \%data%, \%id%, dep, \%error%}'::varchar[],
    '{varchar, varchar, varchar, varchar, varchar}'::varchar[],
    --
    'cvtest',
    NULL::varchar,
    False,
    --
    'valid_rst_tbl',
    10,
    10,
    '{val, dep}'::varchar[]
);
</pre>

(3) Also run with module-specific cross validation function
<pre>
select madlib.ridge_newton_cv(
    'cvtest'::varchar,
    NULL::varchar,
    False,
    'val'::varchar,
    'dep'::varchar,
    '{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290,300,310,320,330,340,350,360,370,380,390,400,410,420,430,440,450,460,470,480,490,500}'::double precision[],
    False,
    10,
    10,
    'valid_rst_tbl1'::varchar
);
</pre>

@sa File cross_validation.sql_in documenting the SQL functions.

*/

------------------------------------------------------------------------
/*
    Generate random remporary names for temp table and other names
*/
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__cv_unique_string ()
RETURNS VARCHAR AS $$
DECLARE
    name        VARCHAR;
BEGIN
    name := '__madlib_temp_'
            || floor(random()*100000000)
            || '_'
            || round(extract(epoch from now()))
            || '_'
            || round(extract(epoch from now()))::INTEGER
                % floor(random() * 100000000)::INTEGER
            || '__';
    return name;
END;
$$ LANGUAGE plpgsql VOLATILE;

------------------------------------------------------------------------
/*
    Given an array of strings, pick out the column names and form a single
    string, so that we could select only the necessary data when copying is
    inevitable.
*/
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__cv_produce_col_name_string (
    tbl         VARCHAR,
    cols        VARCHAR[]
) RETURNS VARCHAR AS $$
    rst = ""
    for i in range(len(cols)):
        rst += tbl + "." + cols[i]
        if i != len(cols) - 1:
            rst += ", "
    return rst
$$ LANGUAGE plpythonu;

------------------------------------------------------------------------
/*
    Create an random ID column for a given data table.

    If the user provides an ID column, which can uniquely identify the rows,
    create a table which maps the row ID to a random integer, which is
    then used as the ID when splitting data.

    If the ID provided by user is already random, then there is no need to
    call this function.
*/
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__cv_generate_random_id(
    rel_origin      VARCHAR,    -- the original data table
    col_id          VARCHAR,    -- name of ID column provided by the user
    rel_random_id   VARCHAR,    -- table for mapping id to random ID
    random_id       VARCHAR,    -- column name for random ID
    origin_id       VARCHAR     -- column name for the orginal non-random ID in table rel_random_id
) RETURNS VOID AS $$
BEGIN
    EXECUTE '
        DROP TABLE IF EXISTS '|| rel_random_id ||';
        CREATE TEMP TABLE '|| rel_random_id ||' AS
            SELECT
                row_number() OVER (ORDER BY random()) AS '|| random_id ||',
                '|| col_id ||' AS '|| origin_id ||'
            FROM
                '|| rel_origin ||'
    ';
END;
$$ LANGUAGE plpgsql VOLATILE;

------------------------------------------------------------------------
/*
    If the user does not provide a ID column, the data table has to be
    copied and at the same time create a random ID column with it.
*/
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__cv_copy_data_with_id(
    rel_origin      VARCHAR,    -- the original data table
    col_data        VARCHAR[],  -- ind & dep data column names as an array
    rel_copied      VARCHAR,    -- the copied table together with a newly generated random ID column
    random_id       VARCHAR     -- the name of the newly generated random ID column
) RETURNS VOID AS $$
DECLARE
    col_string      VARCHAR;
BEGIN
    -- We want to select only the columns that will be used in the computation.
    col_string := MADLIB_SCHEMA.__cv_produce_col_name_string(rel_origin, col_data);
    EXECUTE '
        DROP TABLE IF EXISTS '|| rel_copied ||';
        CREATE TEMP TABLE '|| rel_copied ||' AS
            SELECT
                row_number() OVER (ORDER BY random()) AS '|| random_id ||',
                '|| col_string ||'
            FROM
                '|| rel_origin ||'
    ';
END;
$$ LANGUAGE plpgsql VOLATILE;

------------------------------------------------------------------------
/*
    Compute the start_row and end_row of validation data
    start_row is included, while end_row is not.
*/
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__cv_validation_rows(
    row_num         INTEGER,            -- how many rows in the original data table
    fold_num        DOUBLE PRECISION,   -- k-fold cross validation, ordered by 1_th, 2_th, ..., k_th fold
                                        -- double number is allowed for a more general situation
    which_fold      INTEGER             -- which fold of data is going to be used as validation
) RETURNS INTEGER[] AS $$
DECLARE
    start_row       INTEGER;            -- starts the validation data set from this row (itself included)
    end_row         INTEGER;            -- ends the validation data set at this row (itself not included)
    fold_row_num    INTEGER;            -- each fold's row number
    start_end_pair  INTEGER[];
BEGIN
    fold_row_num := floor(row_num * 1.0 / fold_num); -- how many rows in each cross-validation fold

    -- Since the ID column is already random, a consecutive part of
    -- the ID is enough to extract a good sample of validation data.
    start_row := floor((which_fold - 1) * fold_row_num * 1.0) + 1;
    IF which_fold = fold_num THEN
        end_row :=  row_num + 1; -- the end row itself is not included
    ELSE
        end_row := start_row + fold_row_num;
    END IF;

    SELECT ARRAY[start_row, end_row] INTO start_end_pair;

    RETURN start_end_pair;
END;
$$ LANGUAGE plpgsql VOLATILE;

------------------------------------------------------------------------
/*
    Split the data table according to some given ID column
*/
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__cv_split_data(
    rel_source      VARCHAR,            -- the data table with random IDs
    col_id          VARCHAR,            -- a unique id for each row, it is randomly assigned, starting from 1
    row_num         INTEGER,            -- how many rows in the original data table
    rel_train       VARCHAR,            -- split into training data
    rel_valid       VARCHAR,            -- and validation data
    fold_num        DOUBLE PRECISION,   -- k-fold cross validation, ordered by 1_th, 2_th, ..., k_th fold
                                        -- double number is allowed for a more general situation
    which_fold      INTEGER             -- which fold of data is going to be used as validation
) RETURNS VOID AS $$
DECLARE
    start_row       INTEGER;            -- starts the validation data set from this row (itself included)
    end_row         INTEGER;            -- ends the validation data set at this row (itself not included)
BEGIN
    -- Since the ID column is already random, a consecutive part of
    -- the ID is enough to extract a good sample of validation data.
    SELECT INTO start_row, end_row
        (start_end_pair::INTEGER[])[1],
        (start_end_pair::INTEGER[])[2]
    FROM (
        SELECT
            MADLIB_SCHEMA.__cv_validation_rows(row_num, fold_num, which_fold) AS start_end_pair
        ) AS t;

    -- Extract the training part of data,
    -- which corresponds to rows outside of [start_row, end_row).
    EXECUTE '
        DROP TABLE IF EXISTS '|| rel_train ||';
        CREATE TEMP TABLE '|| rel_train ||' AS
            SELECT * FROM '|| rel_source ||' -- include the ID column
            WHERE '|| col_id ||' < '|| start_row ||'
                OR '|| col_id ||' >= '|| end_row ||'
    ';

    -- Extract the validation part of data,
    -- which corresponds to rows inside of [start_row, end_row).
    EXECUTE '
        DROP TABLE IF EXISTS '|| rel_valid ||';
        CREATE TEMP TABLE '|| rel_valid ||' AS
            SELECT * FROM '|| rel_source ||' -- include the ID column
            WHERE '|| col_id ||' >= '|| start_row ||'
                AND '|| col_id ||' < '|| end_row ||'
    ';
END;
$$ LANGUAGE plpgsql VOLATILE;

------------------------------------------------------------------------
/*
    Split data according to a separated ID mapping table,
    which is generated by the function __generate_random_id
*/
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__cv_split_data(
    rel_origin      VARCHAR,            -- original data
    rel_random_id   VARCHAR,            -- table for mapping id to random ID
    random_id       VARCHAR,            -- column name for random ID
    origin_id       VARCHAR,             -- column name for the original non-random ID
    row_num         INTEGER,            -- how many rows in the original data table
    rel_train       VARCHAR,            -- split into training data
    rel_valid       VARCHAR,            -- and validation data
    fold_num        DOUBLE PRECISION,   -- k-fold cross validation, ordered by 1_th, 2_th, ..., k_th fold
    which_fold      INTEGER             -- which fold of data is going to be used as validation
) RETURNS VOID AS $$
DECLARE
    start_row       INTEGER;            -- starts the validation data set from this row (itself included)
    end_row         INTEGER;            -- ends the validation data set at this row (itself not included)
BEGIN
    -- Since the ID column is already random, a consecutive part of
    -- the ID is enough to extract a good sample of validation data.
    SELECT INTO start_row, end_row
        (start_end_pair::INTEGER[])[1],
        (start_end_pair::INTEGER[])[2]
    FROM (
        SELECT
            MADLIB_SCHEMA.__validation_rows(row_num, fold_num, which_fold) AS start_end_pair
        ) AS t;
   
    -- Extract the training part of data,
    -- which corresponds to rows outside of [start_row, end_row).
    EXECUTE '
        DROP TABLE IF EXISTS '|| rel_train ||';
        CREATE TEMP TABLE '|| rel_train ||' AS
            SELECT
                '|| rel_random_id ||'.'|| random_id ||', -- include the random ID column
                '|| rel_origin ||'.*
            FROM
                '|| rel_origin ||',
                '|| rel_random_id ||'
            WHERE
                '|| rel_origin ||'.'|| origin_id ||' = '|| rel_random_id ||'.'|| origin_id ||'
                AND
                (
                    '|| rel_random_id ||'.'|| random_id ||' < '|| start_row ||'
                    OR
                    '|| rel_random_id ||'.'|| random_id ||' >= '|| end_row ||'
                )            
    ';

    -- Extract the validation part of data,
    -- which corresponds to rows outside of [start_row, end_row).
    EXECUTE '
        DROP TABLE IF EXISTS '|| rel_valid ||';
        CREATE TEMP TABLE '|| rel_valid ||' AS
            SELECT
                '|| rel_random_id ||'.'|| random_id ||',
                '|| rel_origin ||'.*
            FROM
                '|| rel_origin ||',
                '|| rel_random_id ||'
            WHERE
                '|| rel_origin ||'.'|| origin_id ||' = '|| rel_random_id ||'.'|| origin_id ||'
                AND
                (
                    '|| rel_random_id ||'.'|| random_id ||' >= '|| start_row ||'
                    AND
                    '|| rel_random_id ||'.'|| random_id ||' < '|| end_row ||'
                )
    ';
END;
$$ LANGUAGE plpgsql VOLATILE;

------------------------------------------------------------------------
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__cv_combine_params_type_general(
    params          VARCHAR[],
    params_type     VARCHAR[],
    tbl_data        VARCHAR,
    col_random_id   VARCHAR,
    param_explored  VARCHAR,
    explore_value   VARCHAR,
    tbl_input       VARCHAR,
    tbl_output      VARCHAR
) RETURNS VARCHAR AS $$
    if len(params) != len(params_type):
        plpy.error('Parameter number should be equal to the type number!')
        
    rst = ""
    opts = set(["%data%", "%id%", "%error%", "%model%", "%prediction%", param_explored])
    for i in range(len(params)):
        if params[i] not in opts:
            rst += "\'" + params[i] + "\'::" + params_type[i]
        elif params[i] == param_explored:
            rst += "\'" + explore_value + "\'::" + params_type[i]
        elif params[i] == "%data%":
            rst += "\'" + tbl_data + "\'::" + params_type[i]
        elif params[i] == "%id%":
            rst += "\'" + col_random_id + "\'::" + params_type[i]
        elif params[i] == "%error%" :
            rst += "\'" + tbl_output + "\'::" + params_type[i]
        elif params[i] == "%model%":
            if tbl_input is None:
                rst += "\'" + tbl_output + "\'::" + params_type[i]
            else:
                rst += "\'" + tbl_input + "\'::" + params_type[i]
        elif params[i] == "%prediction%":
            if "%error%" in set(params):
                rst += "\'" + tbl_input + "\'::" + params_type[i]
            else:
                rst += "\'" + tbl_output + "\'::" + params_type[i]

        if i != len(params) - 1:
            rst = rst + ", "
    return rst
$$ LANGUAGE plpythonu;

/*
    Call function
*/
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__cv_funcall_general(
    func            VARCHAR,    -- the function
    params          VARCHAR[],  -- parameters of the function
    params_type     VARCHAR[],  -- parameter types
    tbl_data        VARCHAR,    -- data table name for training or validation
    col_random_id   VARCHAR,    -- ID column name
    param_explored  VARCHAR,    -- which parameter is under study
    explore_value   VARCHAR,    -- the value currently under study
    tbl_input       VARCHAR,    -- possible input table
    tbl_output      VARCHAR     -- output the result of function
) RETURNS VOID AS $$
DECLARE
    arguments   VARCHAR;
BEGIN
    arguments := MADLIB_SCHEMA.__cv_combine_params_type_general(
                    params, params_type, tbl_data, col_random_id,
                    param_explored, explore_value, tbl_input, tbl_output
                );
 
    EXECUTE  'SELECT '|| func ||'('|| arguments ||')';
END;
$$ LANGUAGE plpgsql VOLATILE;

/*
    find the type of exploring parameter
*/
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__cv_param_type_explored(
    params          VARCHAR[],
    params_type     VARCHAR[],
    param_explored   VARCHAR
) RETURNS VARCHAR AS $$
    for i in range(len(params)):
        if params[i] == param_explored:
            return params_type[i]
    return None
$$ LANGUAGE plpythonu;

------------------------------------------------------------------------
/*
    Perform cross validation
*/
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.cross_validation_general(
    modelling_func          VARCHAR,    -- function for setting up the model
    modelling_params        VARCHAR[],  -- parameters for modelling
    modelling_params_type   VARCHAR[],  -- parameter types for modelling
    --
    param_explored          VARCHAR,    -- which parameter will be studied using validation
    explore_values          VARCHAR[],  -- values that will be explored for this parameter
    --
    predict_func            VARCHAR,    -- function for predicting using the model
    predict_params          VARCHAR[],  -- parameters for prediction
    predict_params_type     VARCHAR[],  -- parameter types for prediction
    --
    metric_func             VARCHAR,    -- function that computes the error metric
    metric_params           VARCHAR[],  -- parameters for prediction   
    metric_params_type      VARCHAR[],  -- parameter types for prediction
    --
    data_tbl                VARCHAR,    -- table containing the data, which will be split into training and validation parts
    data_id                 VARCHAR,    -- user provide a unique ID for each row
    id_is_random            BOOLEAN,    -- the ID provided by user is random
    --
    validation_result       VARCHAR,    -- store the result: param values, error, +/-
    --
    fold_num                DOUBLE PRECISION,    -- how many fold validation, default: 10
    upto_fold               INTEGER,    -- how many fold actually will be used, default: 10. If 1, it is just one validation.
    data_cols               VARCHAR[]  -- names of data columns that are going to be used
) RETURNS VOID AS $$
DECLARE
    oldClientMinMessages    VARCHAR;
    tbl_used                VARCHAR; -- table name that will be used
    tbl_all_data            VARCHAR := MADLIB_SCHEMA.__cv_unique_string();  -- if need to copy the data, this is the copied table name
    tbl_train               VARCHAR := MADLIB_SCHEMA.__cv_unique_string();  -- table name for training
    tbl_valid               VARCHAR := MADLIB_SCHEMA.__cv_unique_string();  -- table name for validation
    col_random_id           VARCHAR := MADLIB_SCHEMA.__cv_unique_string();  -- column name for random id
    tbl_random_id           VARCHAR := MADLIB_SCHEMA.__cv_unique_string();  -- table for random ID mapping
    row_num                 INTEGER;
    explore_value           VARCHAR;
    explore_type            VARCHAR;
    accum_count             INTEGER;
    tbl_output_model        VARCHAR := MADLIB_SCHEMA.__cv_unique_string();  -- table to store model information
    tbl_output_pred         VARCHAR := MADLIB_SCHEMA.__cv_unique_string();  -- table to store model predictions
    tbl_output_error        VARCHAR := MADLIB_SCHEMA.__cv_unique_string();  -- table to store error for each validation
    tbl_accum_error         VARCHAR := MADLIB_SCHEMA.__cv_unique_string();  -- accumulate the error information
    num_explore_param       INTEGER := array_upper(explore_values, 1);
BEGIN
    oldClientMinMessages :=  (SELECT setting FROM pg_settings WHERE name = 'client_min_messages');
    EXECUTE 'SET client_min_messages TO warning';

    IF data_id IS NULL THEN -- unique ID column is not given, has to copy the data and create the ID
        PERFORM MADLIB_SCHEMA.__cv_copy_data_with_id(data_tbl, data_cols, tbl_all_data, col_random_id);
        tbl_used := tbl_all_data;
    ELSIF id_is_random THEN -- unique ID column is given and is random
        tbl_used := data_tbl; -- nothing needs to be done to the original data table
    ELSE -- the provided unique ID is not random, create a table mapping the given ID to a random ID
        PERFORM MADLIB_SCHEMA.__cv_generate_random_id(data_tbl, data_id, tbl_random_id, col_random_id, data_id);
        tbl_used := data_tbl;
    END IF;
 
    explore_type := MADLIB_SCHEMA.__cv_param_type_explored(modelling_params, modelling_params_type, param_explored);
 
    -- k-fold cross-validation
    EXECUTE 'SELECT count(*) FROM '|| data_tbl INTO row_num;
 
    IF fold_num <= 1 THEN
        RAISE EXCEPTION 'Cross validation total fold number should be larger than 1!';
    END IF;
    
    IF upto_fold < 1 OR upto_fold > fold_num THEN
        RAISE EXCEPTION 'Cannot run with cross validation fold smalled than 1 or larger than total fold number!';
    END IF;

    accum_count := 0;
    FOR k IN 1..upto_fold LOOP
        -- split data into train and validation parts
        IF (data_id IS NULL) OR (data_id IS NOT NULL AND id_is_random) THEN
            PERFORM MADLIB_SCHEMA.__cv_split_data(tbl_used, col_random_id, row_num, tbl_train, tbl_valid, fold_num, k);
        ELSE
            PERFORM MADLIB_SCHEMA.__cv_split_data(tbl_used, tbl_random_id, col_random_id, data_id, row_num,
                                                    tbl_train, tbl_valid, fold_num, k);
        END IF;

        FOR k1 IN 1..num_explore_param LOOP
            explore_value := explore_values[k1];
        
            -- try to be as general as possible
            -- validation using each explore_value
            -- train
            PERFORM MADLIB_SCHEMA.__cv_funcall_general(
                modelling_func, modelling_params, modelling_params_type,
                tbl_train, col_random_id, param_explored, explore_value,
                NULL, tbl_output_model);
   
            -- validate
            PERFORM MADLIB_SCHEMA.__cv_funcall_general(
                predict_func, predict_params, predict_params_type,
                tbl_valid, col_random_id, param_explored, explore_value,
                tbl_output_model, tbl_output_pred);
   
            -- measure the error of the validation part
            PERFORM MADLIB_SCHEMA.__cv_funcall_general(
                metric_func, metric_params, metric_params_type,
                tbl_valid, col_random_id, param_explored, explore_value,
                tbl_output_pred, tbl_output_error);
                
            -- accumulate the measured error result
            accum_count := accum_count + 1;
            IF accum_count = 1 THEN
                EXECUTE '
                    DROP TABLE IF EXISTS '|| tbl_accum_error ||';
                    CREATE TEMP TABLE '|| tbl_accum_error ||' AS
                        SELECT
                            '|| explore_value ||'::'|| explore_type ||' AS '|| param_explored ||',
                            '|| tbl_output_error ||'.*
                        FROM
                            '|| tbl_output_error;
            ELSE
                EXECUTE '
                    INSERT INTO '|| tbl_accum_error ||'
                        SELECT
                            '|| explore_value ||'::'|| explore_type ||' AS '|| param_explored ||',
                            '|| tbl_output_error ||'.*
                        FROM
                            '|| tbl_output_error;
            END IF;
            
        END LOOP;
    END LOOP;

    PERFORM MADLIB_SCHEMA.__cv_summarize_result(tbl_accum_error, validation_result, param_explored);

    -- clean up
    EXECUTE 'DROP TABLE IF EXISTS '|| tbl_all_data;
    EXECUTE 'DROP TABLE IF EXISTS '|| tbl_train;
    EXECUTE 'DROP TABLE IF EXISTS '|| tbl_valid;
    EXECUTE 'DROP TABLE IF EXISTS '|| tbl_random_id;
    EXECUTE 'DROP TABLE IF EXISTS '|| tbl_output_model;
    EXECUTE 'DROP TABLE IF EXISTS '|| tbl_output_pred;
    EXECUTE 'DROP TABLE IF EXISTS '|| tbl_output_error;
    EXECUTE 'DROP TABLE IF EXISTS '|| tbl_accum_error;

    EXECUTE 'SET client_min_messages TO ' || oldClientMinMessages;
END;
$$ LANGUAGE plpgsql VOLATILE;

----------------------------------------------------------------
/*
    summarize the result
*/
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__cv_summarize_result(
    tbl_accum_error     VARCHAR,
    tbl_result          VARCHAR,
    param_explored       VARCHAR
) RETURNS VOID AS $$
DECLARE
    cols        VARCHAR[];
    col         VARCHAR;
    counts      INTEGER;
    sizes       INTEGER;
    num_cols    INTEGER;
BEGIN
    EXECUTE '
        SELECT array_agg(column_name::VARCHAR)
        FROM information_schema.columns
        WHERE table_name = '''|| tbl_accum_error ||''''
        INTO cols;
        
    counts := 0;
    num_cols := array_upper(cols, 1);
    FOR k IN 1..num_cols LOOP
        col := cols[k];

        counts := counts + 1;
        IF counts = 1 THEN
            EXECUTE '
                DROP TABLE IF EXISTS '|| tbl_result ||';
                CREATE TABLE '|| tbl_result ||' AS
                    SELECT '|| param_explored ||'
                    FROM '|| tbl_accum_error ||'
                    GROUP BY '|| param_explored ||'
                    ORDER BY '|| param_explored;
        ELSE
             EXECUTE 'ALTER TABLE '|| tbl_result ||' ADD COLUMN '|| col ||'_avg DOUBLE PRECISION DEFAULT 0';
             EXECUTE '
                UPDATE '|| tbl_result ||' SET '|| col ||'_avg = avgs FROM (
                         SELECT
                             '|| param_explored ||',
                             avg('|| col ||') as avgs
                         FROM '|| tbl_accum_error ||'
                         GROUP BY '|| param_explored ||'
                     ) t
                     WHERE t.'|| param_explored ||' = '|| tbl_result ||'.'|| param_explored;-- ||';
              EXECUTE 'ALTER TABLE '|| tbl_result ||' ADD COLUMN '|| col ||'_stddev DOUBLE PRECISION DEFAULT 0';
              EXECUTE '
                UPDATE '|| tbl_result ||'
                    SET '|| col ||'_stddev = stds
                    FROM (
                        SELECT
                            '|| param_explored ||',
                            stddev('|| col ||') as stds
                        FROM '|| tbl_accum_error ||'
                        GROUP BY '|| param_explored ||'
                    ) t
                    WHERE t.'|| param_explored ||' = '|| tbl_result ||'.'|| param_explored;
        END IF;
    END LOOP;
END;
$$ LANGUAGE plpgsql VOLATILE;

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.cross_validation_general(
    modelling_func          VARCHAR,    -- function for setting up the model
    modelling_params        VARCHAR[],  -- parameters for modelling
    modelling_params_type   VARCHAR[],  -- parameter types for modelling
    --
    param_explored          VARCHAR,    -- which parameter will be studied using validation
    explore_values          VARCHAR[],  -- values that will be explored for this parameter
    --
    predict_func            VARCHAR,    -- function for predicting using the model
    predict_params          VARCHAR[],  -- parameters for prediction
    predict_params_type     VARCHAR[],  -- parameter types for prediction
    --
    metric_func             VARCHAR,    -- function that computes the error metric
    metric_params           VARCHAR[],  -- parameters for prediction   
    metric_params_type      VARCHAR[],  -- parameter types for prediction
    --
    data_tbl                VARCHAR,    -- table containing the data, which will be split into training and validation parts
    data_id                 VARCHAR,    -- user provide a unique ID for each row
    id_is_random            BOOLEAN,    -- the ID provided by user is random
    --
    validation_result       VARCHAR,    -- store the result: param values, error, +/-
    --
    fold_num                DOUBLE PRECISION,    -- how many fold validation, default: 10
    upto_fold               INTEGER    -- how many fold actually will be used, default: 10. If 1, it is just one validation.
) RETURNS VOID AS $$
BEGIN
    IF data_id is NULL THEN
        RAISE EXCEPTION 'The user needs to provide the data columns that are used if no unique ID is associated with each row!';
    ELSE
        PERFORM MADLIB_SCHEMA.cross_validation_general($1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,$16,$17,NULL);
    END IF;
END;
$$ LANGUAGE plpgsql VOLATILE;

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.cross_validation_general(
    modelling_func          VARCHAR,    -- function for setting up the model
    modelling_params        VARCHAR[],  -- parameters for modelling
    modelling_params_type   VARCHAR[],  -- parameter types for modelling
    --
    param_explored          VARCHAR,    -- which parameter will be studied using validation
    explore_values          VARCHAR[],  -- values that will be explored for this parameter
    --
    predict_func            VARCHAR,    -- function for predicting using the model
    predict_params          VARCHAR[],  -- parameters for prediction
    predict_params_type     VARCHAR[],  -- parameter types for prediction
    --
    metric_func             VARCHAR,    -- function that computes the error metric
    metric_params           VARCHAR[],  -- parameters for prediction   
    metric_params_type      VARCHAR[],  -- parameter types for prediction
    --
    data_tbl                VARCHAR,    -- table containing the data, which will be split into training and validation parts
    data_id                 VARCHAR,    -- user provide a unique ID for each row
    id_is_random            BOOLEAN,    -- the ID provided by user is random
    --
    validation_result       VARCHAR,    -- store the result: param values, error, +/-
    --
    fold_num                DOUBLE PRECISION    -- how many fold validation, default: 10
) RETURNS VOID AS $$
BEGIN
    PERFORM MADLIB_SCHEMA.cross_validation_general($1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,$16,($16)::INTEGER);
END;
$$ LANGUAGE plpgsql VOLATILE;

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.cross_validation_general(
    modelling_func          VARCHAR,    -- function for setting up the model
    modelling_params        VARCHAR[],  -- parameters for modelling
    modelling_params_type   VARCHAR[],  -- parameter types for modelling
    --
    param_explored          VARCHAR,    -- which parameter will be studied using validation
    explore_values          VARCHAR[],  -- values that will be explored for this parameter
    --
    predict_func            VARCHAR,    -- function for predicting using the model
    predict_params          VARCHAR[],  -- parameters for prediction
    predict_params_type     VARCHAR[],  -- parameter types for prediction
    --
    metric_func             VARCHAR,    -- function that computes the error metric
    metric_params           VARCHAR[],  -- parameters for prediction   
    metric_params_type      VARCHAR[],  -- parameter types for prediction
    --
    data_tbl                VARCHAR,    -- table containing the data, which will be split into training and validation parts
    data_id                 VARCHAR,    -- user provide a unique ID for each row
    id_is_random            BOOLEAN,    -- the ID provided by user is random
    --
    validation_result       VARCHAR     -- store the result: param values, error, +/-
) RETURNS VOID AS $$
BEGIN
    PERFORM MADLIB_SCHEMA.cross_validation_general($1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,10);
END;
$$ LANGUAGE plpgsql VOLATILE;

------------------------------------------------------------------------
------------------------------------------------------------------------
------------------------------------------------------------------------

CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.cross_validation(
    module_name         VARCHAR,
    args                VARCHAR[],
    tbl_data            VARCHAR,
    data_id             VARCHAR,
    id_is_random        BOOLEAN,
    validation_result   VARCHAR,
    fold_num            INTEGER
) RETURNS VOID AS $$
BEGIN
    IF module_name = 'ridge' THEN
        PERFORM MADLIB_SCHEMA.cross_validation_general(MADLIB_SCHEMA.__ridge_cv_args(args, tbl_data, data_id, id_is_random,
                                                                                    validation_result, fold_num));
    END IF;

    IF module_name = 'lasso' THEN
        PERFORM MADLIB_SCHEMA.cross_validation_general(MADLIB_SCHEMA.__lasso_cv_args(args, tbl_data, data_id, id_is_random,
                                                                                    validation_result, fold_num));
    END IF;
END;
$$ LANGUAGE plpgsql VOLATILE;

