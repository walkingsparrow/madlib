"""
@file lda.py_in 

@brief Latent Dirichlet Allocation inference using collapsed Gibbs
sampling algorithm

@namespace lda

LDA: Driver and auxiliary functions
"""

import plpy
import math
import time

# use mad_vec to process arrays passed as strings in GPDB < 4.1 and PG < 9.0
from utilities.utilities import __mad_version
version_wrapper = __mad_version()
string_to_array = version_wrapper.select_vecfunc()
array_to_string = version_wrapper.select_vec_return()

"""
@brief This class defines a LDATrainer.
"""
class LDATrainer:
    def __init__(
        self, madlib_schema, data_table, model_table, output_data_table,
        voc_size, topic_num, iter_num, alpha, beta): 
        self.madlib_schema = madlib_schema
        self.data_table = data_table
        self.voc_size = voc_size
        self.topic_num = topic_num
        self.iter_num = iter_num
        self.alpha = alpha
        self.beta = beta
        self.model_table = model_table
        self.output_data_table = output_data_table
        self.work_table_0 = '__work_table_train_0__'
        self.work_table_1 = '__work_table_train_1__'

        plpy.execute('DROP TABLE IF EXISTS %s' % (self.work_table_0))
        plpy.execute("""
            CREATE TEMP TABLE %s(
                docid       INT4, 
                wordcount   INT4, 
                words       INT4[], 
                counts      INT4[], 
                doc_topic   INT4[]
                )
                m4_ifdef(`__GREENPLUM__', 
                    `WITH (APPENDONLY=TRUE,COMPRESSTYPE=QUICKLZ) DISTRIBUTED BY
                    (docid)') 
            """ % (self.work_table_0)) 

        plpy.execute('DROP TABLE IF EXISTS %s' % (self.work_table_1))
        plpy.execute("""
            CREATE TEMP TABLE %s(
                docid       INT4, 
                wordcount   INT4, 
                words       INT4[], 
                counts      INT4[], 
                doc_topic   INT4[]
                )
                m4_ifdef(`__GREENPLUM__', 
                    `WITH (APPENDONLY=TRUE,COMPRESSTYPE=QUICKLZ) DISTRIBUTED BY
                    (docid)') 
            """ % (self.work_table_1)) 

        plpy.execute('DROP TABLE IF EXISTS %s' % (self.model_table))
        plpy.execute("""
            CREATE TABLE %s(
                voc_size        INT4, 
                topic_num       INT4, 
                alpha           FLOAT8, 
                beta            FLOAT8, 
                model      INT4[][] 
                )
                m4_ifdef(`__GREENPLUM__', 
                    `WITH (APPENDONLY=TRUE,COMPRESSTYPE=QUICKLZ) DISTRIBUTED
                    RANDOMLY') 
            """ % (self.model_table)) 

        plpy.execute('DROP TABLE IF EXISTS %s' % (self.output_data_table))
        plpy.execute("""
            CREATE TABLE %s(
                docid               INT4, 
                wordcount           INT4, 
                words               INT4[],
                counts              INT4[],
                topic_count         INT4[],
                topic_assignment    INT4[]
                ) 
                m4_ifdef(`__GREENPLUM__', 
                    `WITH (APPENDONLY=TRUE,COMPRESSTYPE=QUICKLZ) DISTRIBUTED BY
                    (docid)') 
            """ % (self.output_data_table)) 

    def init_random(self):
        stime = time.time()
        plpy.notice('initializing topics randomly ...')

        plpy.execute('TRUNCATE TABLE %s' % (self.work_table_0));
        plpy.execute("""
            INSERT INTO {work_table}
            SELECT
                docid, wordcount, words, counts, 
                {madlib_schema}.__lda_random_assign(wordcount, {topic_num}) AS topics
            FROM {data_table}
            """.format(
                work_table = self.work_table_0, 
                madlib_schema = self.madlib_schema, 
                topic_num = self.topic_num,
                data_table = self.data_table)
            )

        etime = time.time()
        plpy.notice('\t\ttime elapsed: %.2f seconds' % (etime - stime))

    def gen_model(self):
        stime = time.time()
        plpy.notice('\t\tgenerating models ...')

        work_table_final = self.work_table_1
        if self.iter_num % 2 == 0:
            work_table_final = self.work_table_0

        plpy.execute('TRUNCATE TABLE %s' % (self.model_table))
        plpy.execute("""
            INSERT INTO %s
            SELECT 
                %d, %d, %f, %f, 
                %s.__lda_count_topic_agg(
                    words, counts, doc_topic[%d:array_upper(doc_topic, 1)], %d,
                    %d) model 
            FROM
                %s 
            """ % (
                self.model_table, self.voc_size, self.topic_num, self.alpha,
                self.beta, self.madlib_schema, self.topic_num + 1,
                self.voc_size, self.topic_num, work_table_final)
            )

        etime = time.time()
        plpy.notice('\t\t\ttime elapsed: %.2f seconds' % (etime - stime))

    def gen_output_data_table(self):
        stime = time.time()
        plpy.notice('\t\tgenerating output data table ...')

        work_table_final = self.work_table_1
        if self.iter_num % 2 == 0:
            work_table_final = self.work_table_0

        plpy.execute('TRUNCATE TABLE %s' % (self.output_data_table))
        plpy.execute("""
            INSERT INTO %s
            SELECT 
                docid, wordcount, words, counts, doc_topic[1:%d] topic_count,
                doc_topic[%d:array_upper(doc_topic,1)] topic_assignment
            FROM
                %s work_table
            """ % (self.output_data_table, self.topic_num, self.topic_num + 1,
            work_table_final))

        etime = time.time()
        plpy.notice('\t\t\ttime elapsed: %.2f seconds' % (etime - stime))

    def iteration(self, it):  
        stime = time.time()

        work_table_in = self.work_table_0
        if it % 2 == 0 :
            work_table_in = self.work_table_1

        work_table_out = self.work_table_1
        if it % 2 == 0 :
            work_table_out = self.work_table_0

        plpy.notice('iteration [%d] ...' % (it))
        plpy.notice('\t\tjoining & sampling ...')
        plpy.execute('TRUNCATE TABLE %s' % (work_table_out))
        query = """
            INSERT INTO {work_table_out}
            SELECT  
                docid, wordcount, words, counts,  
                {madlib_schema}.__lda_gibbs_sample(
                    words, counts, doc_topic, model, 
                    {alpha}, {beta}, {voc_size}, {topic_num}, 1)
            FROM
            (
                SELECT
                    data.docid, wordcount, words, counts, doc_topic, model 
                FROM
                (
                    SELECT
                        docid, model
                    FROM
                    (
                        SELECT
                            {madlib_schema}.__lda_count_topic_agg(
                                words, counts,
                                doc_topic[{topic_num} + 1:array_upper(doc_topic, 1)], 
                                {voc_size}, {topic_num}) model 
                        FROM {work_table_in} 
                    ) t1,
                    (
                        SELECT
                            min(docid) docid
                        FROM {work_table_in} 
                        GROUP BY 
                            m4_ifdef(`__GREENPLUM__', `gp_segment_id', `1 = 1')
                    ) t2
                ) chunk 
                RIGHT JOIN {work_table_in} data
                ON (data.docid = chunk.docid)
                ORDER BY docid
            ) jd
            """.format(
                work_table_out = work_table_out, 
                madlib_schema = self.madlib_schema, 
                alpha = self.alpha, 
                beta = self.beta,
                voc_size = self.voc_size, 
                topic_num = self.topic_num,
                work_table_in = work_table_in)
        plpy.execute(query)
        etime = time.time()
        plpy.notice('\t\ttime elapsed: %.2f seconds' % (etime - stime))

    def run(self):
        stime = time.time()
        plpy.notice('start training process ...')

        self.init_random()
        sstime = time.time()
        for it in range(1, self.iter_num + 1):
            self.iteration(it)
        eetime = time.time()
        plpy.notice('\t\titeration done, time elapsed: %.2f seconds' % (eetime - sstime))

        self.gen_model()
        self.gen_output_data_table()

        etime = time.time()
        plpy.notice('finished, time elapsed: %.2f seconds' % (etime - stime))

"""
@brief This class defines a LDAPredictor
"""
class LDAPredictor:
    def __init__(
        self, madlib_schema, test_table, model_table, output_table, iter_num):
        self.madlib_schema = madlib_schema
        self.test_table = test_table 
        self.model_table = model_table
        self.iter_num = iter_num

        rv = plpy.execute("""
            SELECT 
                voc_size, topic_num, alpha, beta 
            FROM %s
                """ % (self.model_table)) 
        self.voc_size = rv[0]['voc_size']
        self.topic_num = rv[0]['topic_num']
        self.alpha = rv[0]['alpha']
        self.beta = rv[0]['beta']

        self.doc_topic = output_table
        self.work_table_0 = '__work_table_pred_0__'
        self.work_table_1 = '__word_table_pred_1__'
        
        plpy.execute('DROP TABLE IF EXISTS %s' % (self.work_table_0)) 
        plpy.execute("""
            CREATE TEMP TABLE %s(
                docid       INT4, 
                wordcount   INT4, 
                words       INT4[], 
                counts      INT4[], 
                doc_topic   INT4[]
                )
                m4_ifdef(`__GREENPLUM__', 
                    `WITH (APPENDONLY=TRUE,COMPRESSTYPE=QUICKLZ) DISTRIBUTED BY
                    (docid)') 
            """ % (self.work_table_0)) 

        plpy.execute('DROP TABLE IF EXISTS %s' % (self.work_table_1))
        plpy.execute("""
            CREATE TEMP TABLE %s(
                docid       INT4, 
                wordcount   INT4, 
                words       INT4[], 
                counts      INT4[], 
                doc_topic   INT4[]
                )
                m4_ifdef(`__GREENPLUM__', 
                    `WITH (APPENDONLY=TRUE,COMPRESSTYPE=QUICKLZ) DISTRIBUTED BY
                    (docid)') 
            """ % (self.work_table_1)) 

        plpy.execute('DROP TABLE IF EXISTS %s' % self.doc_topic)
        plpy.execute("""
            CREATE TABLE %s(
                docid               INT4, 
                wordcount           INT4,
                words               INT4[],
                counts              INT4[],
                topic_count         INT4[], 
                topic_assignment    INT4[]
                )
                m4_ifdef(`__GREENPLUM__', 
                    `WITH (APPENDONLY=TRUE,COMPRESSTYPE=QUICKLZ) DISTRIBUTED BY
                    (docid)') 
            """ % (self.doc_topic)) 

    def init_random(self):
        stime = time.time()
        plpy.notice('initializing topics randomly ...')

        plpy.execute('TRUNCATE TABLE %s' % (self.work_table_0));
        plpy.execute("""
            INSERT INTO {work_table}
            SELECT
                docid, wordcount, words, counts, 
                {madlib_schema}.__lda_random_assign(wordcount, {topic_num}) AS topics
            FROM {data_table}
            """.format(
                work_table = self.work_table_0, 
                madlib_schema = self.madlib_schema, 
                topic_num = self.topic_num,
                data_table = self.test_table)
            )

        etime = time.time()
        plpy.notice('\t\ttime elapsed: %.2f seconds' % (etime - stime))

    def gen_output_table(self):
        plpy.execute('TRUNCATE TABLE %s' % (self.doc_topic))
        plpy.execute("""
            INSERT INTO %s
            SELECT 
                docid, wordcount, words, counts, doc_topic[1:%d] topic_count,
                doc_topic[%d:array_upper(doc_topic,1)] topic_assignment
            FROM %s work_table_out
            """ % (
                self.doc_topic, self.topic_num, self.topic_num + 1,
                self.work_table_1))

    def infer(self):  
        stime = time.time()
        plpy.notice('infering ...')

        query = """
            INSERT INTO {work_table_out}
            SELECT  
                docid, wordcount, words, counts,
                {schema_madlib}.__lda_gibbs_sample(
                    words, counts, doc_topic, model, {alpha}, {beta},
                    {voc_size}, {topic_num}, {iter_num}
                ) 
            FROM
            (
                SELECT 
                    data.docid, wordcount, words, counts, doc_topic, model
                FROM
                (
                    SELECT
                        docid, model
                    FROM
                    (
                        SELECT
                            min(docid) docid
                        FROM
                            {work_table_in}
                        GROUP BY 
                            m4_ifdef(`__GREENPLUM__', `gp_segment_id', `1 = 1')
                    ) t1,
                    {model_table}
                ) chunk
                RIGHT JOIN
                {work_table_in} data
                ON (data.docid = chunk.docid)
                ORDER BY docid ASC
            ) jd
            """.format(
                work_table_out = self.work_table_1, 
                work_table_in = self.work_table_0, 
                model_table = self.model_table, 
                schema_madlib = self.madlib_schema, 
                alpha = self.alpha, 
                beta = self.beta,
                voc_size = self.voc_size,
                topic_num = self.topic_num,
                iter_num = self.iter_num,
            )
        plpy.execute(query)

        etime = time.time()
        plpy.notice('\t\ttime elapsed: %.2f seconds' % (etime - stime))

    def run(self):
        stime = time.time()
        plpy.notice('start prediction process ...')

        self.init_random()
        self.infer()
        self.gen_output_table()

        etime = time.time()
        plpy.notice('finished, time elapsed: %.2f seconds' % (etime - stime))

"""
@brief This function provides the entry for the LDA training process. 
@param madlib_schema        MDALib schema
@param data_table           Training data table
@param voc_size             Size of vocabulary
@param topic_num            Number of topics
@param iter_num             Number of iterations
@param alpha                Dirichlet parameter for per-document topic multinomial
@param beta                 Dirichlet parameter for per-topic word multinomial
@param model_table          Learned model table
@param output_data_table    Output data table
"""
def lda_train(
    madlib_schema, train_table, model_table, output_data_table, voc_size,
    topic_num, iter_num, alpha, beta):

    __assert(
        train_table.strip() != '',  
        'invalid argument:  train_table is not specified')
    __assert(
        model_table.strip() != '',
        'invalid argument:  model_table is not specified')
    __assert(
        output_data_table.strip() != '',
        'invalid argument:  output_data_table is not specified')
    __assert(
        voc_size > 0, 
        'invalid argument: positive integer expected for voc_size')
    __assert(
        topic_num > 0, 
        'invalid argument: positive integer expected for topic_num')
    __assert(
        iter_num >= 0,
        'invalid argument: positive integer expected for iter_num')
    __assert(
        alpha > 0, 
        'invalid argument: positive real expected for alpha')
    __assert(
        beta > 0, 
        'invalid argument: positive real expected for beta')

    __warn(
        voc_size <= 1e5,
        """the voc_size is very large: %d - make sure that the system has
        enough memory or reduce the vocabulary size""" % (voc_size))
    __warn(
        topic_num <= 1e3,
        """the topic_num is large: %d - make sure that the system has enough
        memory or reduce the topic number"""% (topic_num))

    __validate_data_table(train_table, voc_size)
    convt_table = __convert_data_table(madlib_schema, train_table)
    lt = LDATrainer(
        madlib_schema, convt_table, model_table, output_data_table, voc_size,
        topic_num, iter_num, alpha, beta) 
    lt.run()

"""
@brief This function provides the entry for the LDA prediction process.
@param test_table       name of the testing dataset table
@param model_table      name of the model table
@param iter_num         number of iterations
@param output_table     name of output table
"""
def lda_predict(
    madlib_schema, test_table, model_table, output_data_table, iter_num = 20):

    __assert(
        test_table.strip() != '',  
        'invalid argument: test_table is not specified')
    __assert(
        model_table.strip() != '', 
        'invalid argument: model_table is not specified')
    __assert(
        output_data_table.strip() != '',
        'invalid argument: output_data_table is not specified')
    __assert(
        iter_num >= 0, 
        'invalid argument: positive integer expected for iter_num')
    __warn(
        iter_num <= 20,
        """the iter_num is large: %d - a smaller iter_num (e.g. 20) should be
        good enough""" % (iter_num))

    __check_model_table(model_table)
    rv = plpy.execute('SELECT voc_size FROM %s' % (model_table))
    voc_size = rv[0]['voc_size']
    __validate_data_table(test_table, voc_size)    

    convt_table = __convert_data_table(madlib_schema, test_table)
    lp = LDAPredictor(
        madlib_schema, convt_table, model_table, output_data_table, iter_num) 
    lp.run()

"""
@brief Get the per-topic description by top-k words
@param model_table  The model table generated by the training process
@param vocab_table  The vocabulary table
@param top_k        The top k words for topic description
@param desc_table   The output table for storing the per-topic word description
"""
def get_topic_desc(
    madlib_schema, model_table, vocab_table, desc_table, top_k = 15):
    __assert(
        model_table != '' and
        vocab_table != '' and
        desc_table != '',
        'invalid argument: at least one of the table names is not specified')

    __assert(
        top_k > 0,
        'invalid argument: Positive integer expected for top_k')

    __check_model_table(model_table) 
    __check_vocab_table(vocab_table)

    plpy.execute('DROP TABLE IF EXISTS __lda_topic_word_count__')
    plpy.execute("""
        CREATE TEMP TABLE __lda_topic_word_count__(
            topicid INT4, 
            word_count INT4[], 
            beta FLOAT8
        )
        m4_ifdef(
            `__GREENPLUM__', 
            `WITH (APPENDONLY=TRUE, COMPRESSTYPE=QUICKLZ) DISTRIBUTED BY
            (topicid)') 
        """)

    plpy.execute("""
        INSERT INTO __lda_topic_word_count__
        SELECT 
            generate_series(1, topic_num) topicid,
            %s.__lda_util_unnest(
                %s.__lda_util_transpose(model[1:array_upper(model, 1) - 1])) word_count,
            beta
        FROM %s model
        """ % (madlib_schema, madlib_schema, model_table))

    plpy.execute('DROP TABLE IF EXISTS __lda_topic_word_dist__')
    plpy.execute("""
        CREATE TEMP TABLE __lda_topic_word_dist__(
            topicid INT4, 
            word_dist FLOAT[]
        )
        m4_ifdef(
            `__GREENPLUM__', 
            `WITH (APPENDONLY=TRUE, COMPRESSTYPE=QUICKLZ) DISTRIBUTED BY
            (topicid)') 
        """)

    plpy.execute("""
        INSERT INTO __lda_topic_word_dist__
        SELECT 
            topicid, 
            %s.__lda_util_norm_with_smoothing(word_count, beta) dist 
        FROM
            __lda_topic_word_count__
        """ % (madlib_schema))

    plpy.execute('DROP TABLE IF EXISTS %s' % (desc_table))
    plpy.execute("""
        CREATE TABLE %s (
            topicid INT4, 
            wordid INT4, 
            prob FLOAT, 
            word TEXT)
        m4_ifdef(
            `__GREENPLUM__', 
            `WITH (APPENDONLY=TRUE, COMPRESSTYPE=QUICKLZ) DISTRIBUTED BY
            (topicid)') 
        """ % (desc_table))

    plpy.execute("""
        INSERT INTO %s
        SELECT
            topicid, t2.wordid, prob, word
        FROM
        (
            SELECT
                topicid, wordid, prob,
                rank() OVER(PARTITION BY topicid ORDER BY prob DESC) r
            FROM
            (
                SELECT 
                    topicid, 
                    generate_series(0, array_upper(word_dist, 1) - 1) wordid,
                    unnest(word_dist) prob
                FROM
                    __lda_topic_word_dist__
            ) t1
        ) t2, %s vocab
        WHERE t2.r < %d AND t2.wordid = vocab.wordid
        """ % (desc_table, vocab_table, top_k))

"""
@brief Get the per-topic word counts from the model table
@param model_table     The model table generated by the training process
@param output_table    The output table for storing the per-topic word counts
"""
def get_topic_word_count(madlib_schema, model_table, output_table):
    __assert(
        model_table != '' and
        output_table != '',
        'invalid argument: at least one of the table names is not specified')

    __check_model_table(model_table) 

    plpy.execute('DROP TABLE IF EXISTS %s' % (output_table))
    plpy.execute("""
        CREATE TABLE %s (
            topicid INT4, 
            word_count INT4[]) 
        m4_ifdef(
            `__GREENPLUM__', 
            `WITH (APPENDONLY=TRUE, COMPRESSTYPE=QUICKLZ) DISTRIBUTED BY
            (topicid)') 
        """ % (output_table)) 

    plpy.execute("""
        INSERT INTO %s 
        SELECT 
            generate_series(1, topic_num) topicid,   
            %s.__lda_util_unnest(
                %s.__lda_util_transpose(model[1:array_upper(model, 1) - 1])) word_count
        FROM %s model
        """ % (output_table, madlib_schema, madlib_schema, model_table))

"""
@brief Get the per-word topic counts from the model table
@param model_table     The model table generated by the training process
@param output_table    The output table for storing the per-word topic counts
"""
def get_word_topic_count(madlib_schema, model_table, output_table):
    __assert(
        model_table != '' and
        output_table != '',
        'invalid argument: at least one of the table names is not specified')

    __check_model_table(model_table) 

    plpy.execute('DROP TABLE IF EXISTS %s' % (output_table))
    plpy.execute("""
        CREATE TABLE %s ( 
            wordid INT4, 
            topic_count INT4[]) 
        m4_ifdef(
            `__GREENPLUM__', 
            `WITH (APPENDONLY=TRUE, COMPRESSTYPE=QUICKLZ) DISTRIBUTED BY
            (wordid)') 
        """ % (output_table)) 

    plpy.execute("""
        INSERT INTO %s 
        SELECT 
            generate_series(0, voc_size - 1) wordid,   
            %s.__lda_util_unnest(model[1:array_upper(model, 1) - 1]) topic_count
        FROM %s
        """ % (output_table, madlib_schema, model_table))


"""
@brief Get the perplexity given the prediction and model.
@param model_table     The model table generated by lda_train
@param output_data_table    The output data table generated by lda_predict
"""
def get_perplexity(madlib_schema, model_table, output_data_table):
    __assert(
        model_table != '' and
        output_data_table != '',
        'invalid argument: at least one of the table names is not specified')

    __check_model_table(model_table) 
    params = plpy.execute("""
        SELECT topic_num, voc_size, alpha, beta FROM %s
        """ % (model_table))[0]
    topic_num = params['topic_num']
    voc_size = params['voc_size']
    alpha = params['alpha']
    beta = params['beta']
    __check_output_data_table(output_data_table, topic_num)

    query = """
        SELECT exp(-part_perp/total_word) perp
        FROM
        (
            SELECT {schema_madlib}.__lda_perplexity_agg(
                words, counts, topic_count, model, 
                {alpha}, {beta}, {voc_size}, {topic_num}) part_perp
            FROM
            (
                SELECT 
                    data.docid, words, counts, topic_count, model
                FROM
                (
                    SELECT
                        docid, model
                    FROM
                    (
                        SELECT
                            MIN(docid) docid
                        FROM
                            {out_data_table}
                        GROUP BY
                            m4_ifdef(`__GREENPLUM__', `gp_segment_id', `1 = 1')
                    ) t1, {model_table}
                ) chunk --for join efficiency
                RIGHT JOIN
                    {out_data_table} data 
                ON (data.docid = chunk.docid)
                ORDER BY docid ASC --first model not null
            ) jd
        ) t2,
        (
            SELECT sum(wordcount) total_word FROM {out_data_table}
        ) t3
        """.format(
            schema_madlib = madlib_schema,
            out_data_table = output_data_table, 
            model_table = model_table,
            alpha = alpha,
            beta = beta,
            topic_num = topic_num,
            voc_size = voc_size
        )
    rv = plpy.execute(query)
    return rv[0]['perp']

"""
@brief Normalize the vector using L1 norm with smoothing
@param vector   The vector to be normalized
@param smooth   The smoothing parameter
@return         The normalized vector
"""
def l1_norm_with_smoothing(vector, smooth):
    # special processing of arrays for GPDB < 4.1 and PG < 9.0
    vector = string_to_array(vector, False) 
    norm = sum(map(lambda r: abs(r), vector))
    norm += abs(smooth) * len(vector)
    return array_to_string(
        map(lambda r: float(r + abs(smooth)) / norm, vector))

"""
@brief Checks the vocabulary and converts non-continous wordids into continuous
integers ranging from 0 to voc_size - 1. 
@param vocab_table  The vocabulary table in the form of 
                    <wordid::INT4, word::text>
@param out_table    The normalized vocabulary table 
"""
def norm_vocab(vocab_table, out_table):
    __check_vocab_table(vocab_table)
    plpy.execute('DROP TABLE IF EXISTS %s' % out_table)
    plpy.execute("""
        CREATE TABLE %s (
            wordid INT4, 
            old_wordid INT4,
            word TEXT
        )
        m4_ifdef(`__GREENPLUM__', `WITH(APPENDONLY=TRUE,COMPRESSTYPE=QUICKLZ)
        DISTRIBUTED BY (wordid)')
        """ % (out_table))

    plpy.execute("""
        INSERT INTO %s
        SELECT r - 1, wordid, word
        FROM
        (
            SELECT
                wordid, word, rank() OVER(ORDER BY wordid) r
            FROM
            (
                SELECT wordid, word
                FROM
                (
                    SELECT 
                        wordid, 
                        word, 
                        rank() OVER(PARTITION BY wordid ORDER BY word ASC) r
                    FROM 
                        %s voc 
                ) t1
                WHERE r = 1
            ) t2
        ) t3
        """ % (out_table, vocab_table))

"""
@brief Normalize the data table according to the normalized vocabulary, rows
with non-positive count values will be removed
@param data_table   The data table to be normalized
@param vocab_table  The nomralized vocabulary table
@param output_table The normalized data table
"""
def norm_dataset(data_table, vocab_table, output_table):
    __check_data_table(data_table)    
    __check_norm_vocab_table(vocab_table)
    plpy.execute('DROP TABLE IF EXISTS %s' % (output_table))
    plpy.execute("""
        CREATE TABLE %s (
            docid INT4, 
            wordid INT4, 
            count INT4
        )
        m4_ifdef(`__GREENPLUM__', `WITH(APPENDONLY=TRUE,COMPRESSTYPE=QUICKLZ)
        DISTRIBUTED BY (docid)')
        """ % (output_table))

    plpy.execute("""
        INSERT INTO %s
        SELECT 
            docid, 
            vocab.wordid, 
            count
        FROM
            %s data, 
            %s vocab
        WHERE 
            data.wordid = vocab.old_wordid AND data.count > 0 
        """ % (output_table, data_table, vocab_table))

"""
@brief Co-normalize the data table and the vocabulary table
@param data_table           The data table to be normalized
@param vocab_table          The vocabulary table to be nomralized
@param output_data_table    The normalized data table
@param output_vocab_table   The normalized vocabulary table
"""
def conorm_data(
    data_table, vocab_table, output_data_table, output_vocab_table):
    __check_data_table(data_table)
    __check_vocab_table(vocab_table)

    plpy.execute('DROP TABLE IF EXISTS %s' % (output_data_table))
    plpy.execute("""
        CREATE TABLE %s (
            docid   INT4, 
            wordid  INT4, 
            count   INT4
        )
        m4_ifdef(`__GREENPLUM__', `WITH(APPENDONLY=TRUE,COMPRESSTYPE=QUICKLZ)
        DISTRIBUTED BY (docid)')
        """ % (output_data_table))

    plpy.execute('DROP TABLE IF EXISTS %s' % (output_vocab_table))
    plpy.execute("""
        CREATE TABLE %s (
            wordid      INT4, 
            old_wordid  INT4,
            word        TEXT
        )
        m4_ifdef(`__GREENPLUM__', `WITH(APPENDONLY=TRUE,COMPRESSTYPE=QUICKLZ)
        DISTRIBUTED BY (wordid)')
        """ % (output_vocab_table))

    plpy.execute('DROP TABLE IF EXISTS __vocab__')
    plpy.execute("""
        CREATE TEMP TABLE __vocab__(
            wordid  INT4, 
            word    TEXT
        )
        m4_ifdef(`__GREENPLUM__', `WITH(APPENDONLY=TRUE,COMPRESSTYPE=QUICKLZ)
        DISTRIBUTED BY (wordid)')
        """) 

    plpy.execute("""
        INSERT INTO __vocab__
        SELECT voc.wordid, voc.word
        FROM
        (
            SELECT
                wordid 
            FROM
                %s data 
            GROUP BY wordid
        ) tvoc,
        %s voc
        WHERE tvoc.wordid = voc.wordid
        """ % (data_table, vocab_table))
    norm_vocab('__vocab__', output_vocab_table)
    norm_dataset(data_table, output_vocab_table, output_data_table)

"""
@brief Return the index of elements in a sorted order
@param vector   The array to be sorted
@return         The index of elements
"""
def index_sort(vector):
    # process arrays for GPDB < 4.1 and PG < 9.0
    vector = string_to_array(vector, False)
    plpy.warning(str(vector))
    dim = len(vector)
    idx = range(dim)
    idx.sort(key = lambda r: vector[r])
    plpy.warning(str(idx))
    return array_to_string(map(lambda r: r + 1, idx))

"""
@brief Convert the format of data table from <docid, wordid, count> to <docid,
wordcount, words, counts>.
@param data_table   The data table to be converted
@param return       The converted table name
"""
def __convert_data_table(madlib_schema, data_table):
    plpy.notice('converting the data table ...')
    convt_table = '__lda_convt_corpus__'
    plpy.execute('DROP TABLE IF EXISTS %s' % (convt_table))
    plpy.execute("""
        CREATE TEMP TABLE %s
        (
            docid INT4,
            wordcount INT4,
            words INT4[],
            counts INT4[]
        )
        m4_ifdef(`__GREENPLUM__', `WITH(APPENDONLY=TRUE,COMPRESSTYPE=QUICKLZ)
        DISTRIBUTED BY (docid)')
        """ % (convt_table))

    plpy.execute("""
        INSERT INTO %s
        SELECT 
            docid, 
            sum(count) wordcount,
            %s.array_agg(wordid) words,
            %s.array_agg(count) counts
        FROM
            %s data
        WHERE
            (docid IS NOT NULL) AND 
            (wordid IS NOT NULL) AND 
            (count IS NOT NULL)
        GROUP BY docid
        """ % (convt_table, madlib_schema, madlib_schema, data_table))

    return convt_table

"""
@brief if the given condition is false, then raise an error with the message
@param condition    the condition to be asserted
@param msg          the error message to be reported
"""
def __assert(condition, msg):
    if not condition:
        plpy.error(msg)

"""
@brief if the given condition is false, then raise an warning with the message
@param condition    the condition to be asserted
@param msg          the error message to be reported
"""
def __warn(condition, msg):
    if not condition:
        plpy.warning(msg)

"""
@brief Check the structure of the data table
@param data_table   The data table name
"""
def __check_data_table(data_table):
    plpy.notice('checking the data table ... ')
    assert(data_table.strip() != '', 'no data table is specified')
    try:
        rv = plpy.execute("""
            SELECT count(*) cnt FROM pg_attribute 
            WHERE 
                attrelid = '%s'::regclass AND
                ((atttypid = 'INT4'::regtype AND attname = 'docid') OR
                (atttypid = 'INT4'::regtype AND attname = 'wordid') OR
                (atttypid = 'INT4'::regtype AND attname = 'count'))
            """ % (data_table))
        __assert(
           rv[0]['cnt'] == 3, 
           """the %s shoudl have docid::INT4, wordid::INT4, count::INT4
           columns""" % (data_table))
    except:
        __assert(0,
           """the %s must exist and should have docid, wordid, and count
           columns""" % (data_table))

"""
@brief Check the validity of the data table
@param data_table   The data table
@param voc_size     The size of vocabulary
"""
def __validate_data_table(data_table, voc_size):
    plpy.notice('validating the data table ...')
    __check_data_table(data_table)
    rv = plpy.execute('SELECT count(*) cnt FROM %s WHERE docid < 0' % (data_table))
    __warn(
        rv[0]['cnt'] == 0,
        """%d rows have negative docid - use continous non-negative
        integers for docid for better interpretation""" % (rv[0]['cnt']))

    rv = plpy.execute('SELECT count(*) cnt FROM %s WHERE wordid < 0' % (data_table))
    __assert(
        rv[0]['cnt'] == 0,
        """%d rows have negative wordid - the wordid must range from 0 to
        voc_size - 1""" % (rv[0]['cnt']))

    rv = plpy.execute("""
        SELECT 
            count(wordid) size, 
            min(wordid) min_wordid, 
            max(wordid) max_wordid
        FROM
        (
            SELECT wordid 
            FROM %s
            GROUP BY wordid
        ) t1
        """ % (data_table))
    if 0 == rv[0]['size']:
        plpy.warning('%s is empty' % (data_table))
        return
    __assert(
        rv[0]['size'] <= voc_size,
        """the actual size of vocabulary %d is larger than the specified %d -
        set the correct voc_size and try again""" % (rv[0]['size'], voc_size))
    __assert(
        rv[0]['min_wordid'] <= voc_size - 1 and
        rv[0]['max_wordid'] <= voc_size - 1,
        """the wordid should be in the range of 0 to %d""" % (voc_size - 1))
    __warn(
        rv[0]['size'] == voc_size,
        """the actual size of vocabulary %d is little than the specified %d -
        the vocabulary and dataset normalization is highly recommended for
        memory efficiency""" % (rv[0]['size'], voc_size))
    __warn(
        rv[0]['min_wordid'] == 0,
        """the actual min wordid is large than 0 - the vocabulary and dataset
        normalization is highly recommended for memory efficiency""")
    __warn(
        rv[0]['max_wordid'] == voc_size - 1,
        """the actual max wordid is smaller than the specified (voc_size - 1 =
        %d) - set the voc_size to %d for memory efficiency""" % (voc_size - 1,
        rv[0]['max_wordid'] + 1))

    rv = plpy.execute('SELECT count(*) cnt FROM %s WHERE count <= 0' % (data_table))
    __assert(
        rv[0]['cnt'] == 0,
        """%d rows have zero or negative count - the value in the count column
        must be positive integers""" % (rv[0]['cnt']))

"""
@brief Check the validity of the vocabulary table
@param vocab_table   The vocabulary table name
"""
def __check_vocab_table(vocab_table):
    plpy.notice('checking the vocabulary table ...')
    try:
        rv = plpy.execute("""
            SELECT count(*) cnt FROM pg_attribute 
            WHERE 
                attrelid = '%s'::regclass AND
                ((atttypid = 'INT4'::regtype AND attname = 'wordid') OR
                (atttypid = 'text'::regtype AND attname = 'word'))
            """ % (vocab_table))
        __assert(
           rv[0]['cnt'] == 2, 
           """the %s should have wordid::INT4, word::TEXT columns""" % (vocab_table))
    except:
        __assert(0,
            """the %s must exist and should have wordid and word columnes""" %
            (vocab_table))

"""
@brief Check the validity of the normalized vocabulary table
@param vocab_table   The normalized vocabulary table name
"""
def __check_norm_vocab_table(vocab_table):
    plpy.notice('checking the normalized vocabulary table ...')
    try:
        rv = plpy.execute("""
            SELECT count(*) cnt FROM pg_attribute 
            WHERE 
                attrelid = '%s'::regclass AND
                ((atttypid = 'INT4'::regtype AND attname = 'wordid') OR
                (atttypid = 'INT4'::regtype AND attname = 'old_wordid') OR
                (atttypid = 'text'::regtype AND attname = 'word'))
            """ % (vocab_table))
        __assert(
            rv[0]['cnt'] == 3, 
            """the %s should have wordid, old_wordid, and word columns""" %
            (vocab_table))
    except:
        __assert(0,
           """the %s must exist and should have wordid, old_wordid, and word
           columns""" % (vocab_table))


"""
@brief Check the validity of the output data table
@param model_table  Output data table name
@param topic_num    Topic number
"""
def __check_output_data_table(output_data_table, topic_num):
    plpy.notice('checking the output data table ...')
    try:
        rv = plpy.execute("""
            SELECT count(*) cnt 
            FROM pg_attribute 
            WHERE 
               attrelid = '%s'::regclass AND
               ((atttypid = 'INT4[]'::regtype AND attname = 'words') OR
               (atttypid = 'INT4[]'::regtype AND attname = 'counts') OR
               (atttypid = 'INT4[]'::regtype AND attname = 'topic_count')) 
           """ % (output_data_table))
        __assert(
            rv[0]['cnt'] == 3, 
            """the %s must have words::INT4[], counts::INT4[], and
            topic_count::INT4[] columns""" % (output_data_table))
    except:
        __assert(0,
            """the %s must exist and should have words, counts, and topic_count
            columns""" % (output_data_table))

    rv = plpy.execute("""
        SELECT min(dim) min_dim, max(dim) max_dim
        FROM
        (
            SELECT array_upper(topic_count, 1) dim
            FROM %s
        ) t1
        """ % (output_data_table))
    __assert(
        rv[0]['min_dim'] == rv[0]['max_dim'] and
        rv[0]['min_dim'] == topic_num,
        'Dimension mismatch - array_upper(topic_count, 1) <> topic_num')

"""
@brief Check the validity of the model table
@param model_table  Model table name
"""
def __check_model_table(model_table):
    plpy.notice('checking the model table ...')
    try:
        rv = plpy.execute("""
            SELECT count(*) cnt 
            FROM pg_attribute 
            WHERE 
               attrelid = '%s'::regclass AND
               ((atttypid = 'INT4'::regtype AND attname = 'voc_size') OR
               (atttypid = 'INT4'::regtype AND attname = 'topic_num') OR
               (atttypid = 'FLOAT8'::regtype AND attname = 'alpha') OR
               (atttypid = 'FLOAT8'::regtype AND attname = 'beta') OR
               (atttypid = 'INT4[]'::regtype AND attname = 'model')) 
           """ % (model_table))
        __assert(
            rv[0]['cnt'] == 5, 
            """the %s must have voc_size::INT4, topic_num::INT4, alpha::FLOAT8,
            beta::FLOAT8, and model::INT4[] columns""" % (model_table))
    except:
        __assert(0,
            """the %s must exist and should have voc_size, topic_num, alpha,
            beta, word_topic, and corpus_topic columns""" % (model_table))

    rv = plpy.execute("""
        SELECT voc_size, topic_num, alpha, beta, 
            array_upper(model, 1) * array_upper(model, 2) model_size, 
            array_upper(model, 2) count_size 
        FROM %s
        """ % (model_table))
    __assert(
        len(rv) > 0, 
        '%s is empty' % (model_table))
    __assert(
        len(rv) == 1, 
        'the %s should have only 1 row' % (model_table))
    __assert(
        rv[0]['voc_size'] > 0, 
        'the voc_size in %s should be a positive integer' % (model_table))
    __assert(
        rv[0]['topic_num'] > 0, 
        'the topic_num in %s should be a positive integer' % (model_table))
    __assert(
        rv[0]['alpha'] > 0, 
        'the alpha in %s should be a positive real number' % (model_table))
    __assert(
        rv[0]['beta'] > 0, 
        'the beta in %s should be a positive real number' % (model_table))
    __assert(
        rv[0]['model_size'] == (rv[0]['voc_size'] + 1) * rv[0]['topic_num'], 
        """the model_size mismatchs with voc_size and topic_num in %s""" %
        (model_table)) 
    __assert(
        rv[0]['count_size'] == rv[0]['topic_num'], 
        """the size of topic counts mismatchs with the topic_num in %s""" %
        (model_table))
