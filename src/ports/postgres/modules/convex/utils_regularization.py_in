 
import plpy
import math
from utilities.utilities import __unique_string
from validation.cv_utils import __cv_produce_col_name_string
from validation.cv_utils import __cv_copy_data_with_id
from validation.cv_utils import __cv_split_data_using_id_col
from validation.cv_utils import __cv_split_data_using_id_tbl
from validation.cv_utils import __cv_generate_random_id

## ========================================================================
    
def __utils_ind_var_scales(**kwargs):
    """
    The mean and standard deviation for each element of the independent variable,
    which is an array in ridge and lasso.

    The output will be stored in a temp table: a mean array and a std array.

    This function is also used in lasso.
    """
    scale_factor = math.sqrt(1. - 1./float(kwargs["row_num"]))
    plpy.execute("""
        drop table if exists {tbl_scales};
        create temp table {tbl_scales} as
            select
                attr,
                avg(val) as mean,
                stddev(val) * {scale_factor} as std
            from (
                select
                    generate_series(1, {dimension}) as attr,
                    unnest({col_ind_var}) as val
                from {tbl_data}
            ) t
            group by attr
    """.format(scale_factor = scale_factor, **kwargs))
    return None

## ========================================================================
    
def __utils_dep_var_scale(**kwargs):
    """
    The mean and standard deviation for each element of the dependent variable,
    which is a scalar in ridge and lasso.

    The output will be stored in a temp table: a mean array and a std array

    This function is also used in lasso.
    """
    plpy.execute("""
        drop table if exists {tbl_scale};
        create temp table {tbl_scale} as
            select
                avg({col_dep_var}) as dep_avg,
                1 as dep_std
            from {tbl_data}
    """.format(**kwargs))
    return None

## ========================================================================
    
def __utils_normalize_data(**kwargs):
    """
    Normalize the independent and dependent variables using the calculated mean's and std's
    in __utils_ind_var_scales and __utils_dep_var_scale.

    Compute the scaled variables by: scaled_value = (origin_value - mean) / std, and special
    care is needed if std is zero.
    
    The output is a table with scaled independent and dependent variables.

    This function is also used in lasso.

    Parameters:
    tbl_data -- original data
    col_ind_var -- independent variables column 
    dimension -- length of independent variable array
    col_dep_var -- dependent variable column
    tbl_ind_scales -- independent variables scales array
    tbl_dep_scale -- dependent variable scale 
    tbl_data_scaled -- scaled data result
    """
    # intermediate table name to avoid name conflicts 
    foos = __unique_string()
    plpy.execute("drop table if exists {tbl_data_scaled}".format(**kwargs))
    plpy.execute("""
        create temp table {tbl_data_scaled} as
            select
                array_agg(val order by attr ) as {col_ind_var},
                max(dep_var) as {col_dep_var}
            from (
                select
                    ids,
                    {foos}.attr as attr,
                    (dep_var - dep_avg) / dep_std as dep_var,
                    (
                        case when std = 0 then
                            val - mean
                        else
                            (val - mean) / (case when std = 0 then 1 else std end)
                        end
                    ) as val
                from
                    {tbl_ind_scales},
                    {tbl_dep_scale},
                    (
                        select
                            ids,
                            {col_dep_var} as dep_var,
                            generate_series(1, {dimension}) as attr,
                            unnest({col_ind_var}) as val
                        from (
                            select
                                row_number() over () as ids,
                                {col_dep_var},
                                {col_ind_var}
                            from {tbl_data}) t
                    ) as {foos}
                where {tbl_ind_scales}.attr = {foos}.attr
            ) as t
            group by ids
    """.format(foos = foos, **kwargs))
    return None

## ========================================================================
    
def __utils_gather_results(**kwargs):
    """
    Put normalized fitting result together with scales to be used for nonlinear fitting.

    Non-linear fitting just uses normalized fitting coefficients to get prediction. Restore the coefficients
    to the original scale involves non-linear calculation and is usually not necessary.

    Parameters:
    tbl_coef - table contains the rsult of fitting
    col_coef - column name for the fitting coefficients
    col_others - column names other than the one for the fitting coefficients
    tbl_ind_scales - independent variable scales
    tbl_dep_scale - dependent variable scale
    """
    col_others_string = __cv_produce_col_name_string(kwargs["tbl_coef"], kwargs["col_others"])
    plpy.execute("drop table if exists {tbl_all_results}".format(**kwargs))
    plpy.execute("""
        create table {tbl_all_results} as
            select
                coefficients as {col_coef},
                intercept as intercept,
                array_agg({tbl_ind_scales}.mean order by {tbl_ind_scales}.attr) as ind_var_mean,
                {tbl_ind_scales}.std as ind_var_std,
                {col_others_string}
            from
                {tbl_coef},
                {tbl_ind_scales}
    """.format(col_others_string = col_others_string, **kwargs))
    return None

## ========================================================================
    
def __utils_restore_linear_coef_scales_sql(**kwargs):
    """
    The query string for restoring the fitting coefficients using the calculated mean's and std's
    in __utils_ind_var_scales and __utils_dep_var_scale.

    Only for linear fitting.
    """
    subq2 = __unique_string()
    query = """
        select
            subq1.coefficients as {col_coef},
            subq1.intercept + subq1.dep_avg as intercept
        from (
            select
                array_agg(val order by attr) as coefficients,
                sum(intercept_tmp) as intercept,
                max(dep_avg) as dep_avg
            from (
                select
                    {subq2}.attr,
                    (
                        case when std = 0 then
                            0.
                        else
                            coef * dep_std / (case when std = 0 then 1 else std end)
                        end
                    ) as val,
                    (
                        case when std = 0 then
                            0.
                        else
                            - coef * dep_std * mean / std
                        end
                    ) as intercept_tmp,
                    dep_avg
                from
                    {tbl_ind_scales},
                    {tbl_dep_scale},
                    (
                        select
                            generate_series(1, {dimension}) as attr,
                            unnest({col_coef}) as coef
                        from {tbl_coef}
                    ) as {subq2}
                where {tbl_ind_scales}.attr = {subq2}.attr
            ) as subq3
        ) as subq1
    """.format(subq2 = subq2, **kwargs)
    return query

## ========================================================================
    
def __utils_restore_linear_coef_scales(**kwargs):
    """
    Restore the linear coefficients to the original scale.

    This function is also used in lasso.
    """
    col_others_string = __cv_produce_col_name_string(kwargs["tbl_coef"], kwargs["col_others"])
    foos = __unique_string()
    goos = __unique_string()
    sql_query = __utils_restore_linear_coef_scales_sql(**kwargs)
    plpy.execute("drop table if exists {tbl_origin_coef}".format(**kwargs))
    plpy.execute("""
        create table {tbl_origin_coef} as
            select
                {foos}.{col_coef},
                {foos}.intercept,
                {goos}.ind_var_mean,
                {goos}.ind_var_std,
                {tbl_dep_scale}.dep_avg AS dep_var_mean,
                {tbl_dep_scale}.dep_std AS dep_var_std,
                {col_others_string}
            from
                ({sql_query}) as {foos},
                {tbl_coef},
                (
                    SELECT
                        array_agg({tbl_ind_scales}.mean ORDER BY {tbl_ind_scales}.attr) AS ind_var_mean,
                        array_agg({tbl_ind_scales}.std ORDER BY {tbl_ind_scales}.attr) AS ind_var_std
                    FROM {tbl_ind_scales}
                ) {goos},
                {tbl_dep_scale}
    """.format(col_others_string = col_others_string,
               foos = foos, goos = goos,
               sql_query = sql_query, **kwargs))
    return None

## ========================================================================
    
def __utils_normalization_cv_restore(coef_count, **kwargs):
    """
    Function used by both ridge and lasso cross validation functions:
    
    Restore the fitting coefficients to the original scales without using
    extra table to avoid transaction lock limits.
    """
    sql_query = __utils_restore_linear_coef_scales_sql(
        col_coef = "coef",
        dimension = kwargs["dimension"],
        tbl_ind_scales = kwargs["tbl_ind_scales"],
        tbl_dep_scale = kwargs["tbl_dep_scale"],
        tbl_coef = "{tbl_coef} where id = {coef_count}-1".format(
            coef_count = coef_count,
            **kwargs)
    )                
    plpy.execute("""
        insert into {tbl_coef}
            select
                {coef_count},
                coef,
                intercept
            from (
                {sql_query}
            ) t
    """.format(coef_count = coef_count, sql_query = sql_query,
               **kwargs))

## ========================================================================
    
def __utils_cv_preprocess(kwargs):
    """
    Some common processes used in both ridge and lasso cross validation functions:

    copy data if needed,
    update name space,
    check the validity of fold_num,
    create table to store all the fitting coefficients
    """
    # table containing the data, which will be
    # split into training and validation parts
    data_tbl = kwargs["data_tbl"]
    # whether the user provides a unique ID for each row
    # if not, then it is None
    data_id = kwargs["data_id"]
    # whether the ID provided by user is random
    id_is_random = kwargs["id_is_random"]
    col_ind_var = kwargs["col_ind_var"]
    col_dep_var = kwargs["col_dep_var"]
    # how many fold validation, default: 10
    fold_num = kwargs["fold_num"]
    # how many fold actually will be used, default: 10.
    # If 1, it is just one validation.
    upto_fold = kwargs["upto_fold"]
    # if need to copy the data,
    # this is the copied table name
    tbl_all_data = __unique_string()
    # table name before normalization
    tbl_inter = __unique_string()
    # table name for training
    tbl_train = __unique_string()
    # table name for validation
    tbl_valid = __unique_string()
    # column name for random id
    col_random_id = __unique_string()
    # table for random ID mapping, used when
    # data_id is not None and id_is_random is False
    tbl_random_id = __unique_string()
    # accumulate the error information
    tbl_accum_error = __unique_string()
    # independent variable (array) scales
    # inclduing mean's and std's
    tbl_ind_scales = __unique_string()
    # dependent variable scale including mean and std
    tbl_dep_scale = __unique_string()
    # table to store fitting coefficients from
    # all validations for all parameter values
    tbl_coef = __unique_string() 
    kwargs.update(dict(tbl_accum_error = tbl_accum_error,
                       tbl_all_data = tbl_all_data,
                       tbl_inter = tbl_inter,
                       tbl_train = tbl_train,
                       tbl_valid = tbl_valid,
                       tbl_random_id = tbl_random_id,
                       col_random_id = col_random_id,
                       tbl_ind_scales = tbl_ind_scales,
                       tbl_dep_scale = tbl_dep_scale,
                       tbl_coef = tbl_coef))
    
    data_cols = [col_ind_var, col_dep_var]
    if data_id is None:
        # unique ID column is not given, has to copy the data and create the ID
        __cv_copy_data_with_id(data_tbl, data_cols, tbl_all_data, col_random_id)
        tbl_used = tbl_all_data
    elif id_is_random:
        # unique ID column is given and is random
        tbl_used = data_tbl
        col_random_id = data_id
        kwags["col_random_id"] = data_id
    else:
        # the provided unique ID is not random, create
        # a table mapping the given ID to a random ID
        __cv_generate_random_id(data_tbl, data_id, tbl_random_id, col_random_id)
        tbl_used = data_tbl

    # original data row number
    row_num = plpy.execute("select count(*) as row_num from {data_tbl}".format(**kwargs))[0]["row_num"]
    # length of the independent variable array
    dimension = plpy.execute("select max(array_upper({col_ind_var},1)) as dimension from {data_tbl}".format(**kwargs))[0]["dimension"]

    kwargs.update(dict(tbl_used = tbl_used, row_num = row_num, dimension = dimension))

    # table to append all fitting results
    # which are distinguished by id
    plpy.execute("""
        drop table if exists {tbl_coef};
        create temp table {tbl_coef} (id integer, coef double precision[], intercept double precision)
    """.format(**kwargs))
    return None

## ========================================================================
    
def __utils_cv_split_and_normalization(k, kwargs):
    """
    Another set of common processes needed by both ridge and lasso cross validation functions:

    split the original data into training and validation parts,
    normalize training data
    """
    data_id = kwargs["data_id"]
    id_is_random = kwargs["id_is_random"]
    tbl_used = kwargs["tbl_used"]
    col_random_id = kwargs["col_random_id"]
    row_num = kwargs["row_num"]
    tbl_train = kwargs["tbl_train"]
    tbl_valid = kwargs["tbl_valid"]
    fold_num = kwargs["fold_num"]
    tbl_random_id = kwargs["tbl_random_id"]
    tbl_inter = kwargs["tbl_inter"]
    normalization = kwargs["normalization"]
    dimension = kwargs["dimension"]
    tbl_ind_scales = kwargs["tbl_ind_scales"]
    tbl_dep_scale = kwargs["tbl_dep_scale"]
    col_ind_var = kwargs["col_ind_var"]
    col_dep_var = kwargs["col_dep_var"]

    col_data = [col_ind_var, col_dep_var]
    if (data_id is None) or (data_id is not None and id_is_random):
        __cv_split_data_using_id_col(tbl_used, col_data, col_random_id, row_num, tbl_inter, tbl_valid, fold_num, k+1)
    else:
        __cv_split_data_using_id_tbl(tbl_used, col_data, tbl_random_id, col_random_id, data_id, row_num, tbl_inter, tbl_valid, fold_num, k+1)

    row_num_train = plpy.execute("select count(*) as n from " + tbl_inter)[0]["n"]
    kwargs["row_num_train"] = row_num_train
    
    if normalization:
        __utils_ind_var_scales(tbl_data = tbl_inter, col_ind_var = col_ind_var, row_num = row_num_train, dimension = dimension, tbl_scales = tbl_ind_scales)
        __utils_dep_var_scale(tbl_data = tbl_inter, col_dep_var = col_dep_var, row_num = row_num_train, tbl_scale = tbl_dep_scale)
        __utils_normalize_data(tbl_data = tbl_inter, col_ind_var = col_ind_var, dimension = dimension, col_dep_var = col_dep_var, tbl_ind_scales = tbl_ind_scales, tbl_dep_scale = tbl_dep_scale, tbl_data_scaled = tbl_train)
        # dep_std = plpy.execute("select dep_std as std from " + tbl_dep_scale)[0]["std"]
        kwargs["dep_std"] = 1
    else:
        kwargs["tbl_train"] = tbl_inter

    return None

## ========================================================================
    
def __utils_accumulate_error(accum_count, tbl_accum_error, param_value, error):
    """
    Function needed by both ridge and lasso cross validation functions:

    accumulate measured errors from each validation for each lambda value.
    """
    if accum_count == 1:
        plpy.execute("create temp table {tbl_accum_error} (lambda double precision, mean_squared_error double precision)".format(tbl_accum_error = tbl_accum_error))
    plpy.execute("insert into {tbl_accum_error} values ({param_value}, {error})".format(
        tbl_accum_error = tbl_accum_error,
        param_value = param_value, error = error))
