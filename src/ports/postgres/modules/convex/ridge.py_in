 
import plpy
import math
from utilities.utilities import __unique_string
# from validation.cv_utils import __cv_produce_col_name_string
from validation.cv_utils import __cv_summarize_result
from utilities.validate_args import __is_tbl_exists
from utilities.validate_args import __is_tbl_has_rows
# from utilities.validate_args import __is_col_exists
from utilities.validate_args import __is_tbl_exists_in_schema
from utilities.validate_args import __is_scalar_col_no_null
# from utilities.validate_args import __is_array_col_same_dimension
# from utilities.validate_args import __is_array_col_no_null
from convex.utils_regularization import __utils_restore_linear_coef_scales
from convex.utils_regularization import __utils_normalization_cv_restore
from convex.utils_regularization import __utils_cv_preprocess
from convex.utils_regularization import __utils_cv_split_and_normalization
from convex.utils_regularization import __utils_accumulate_error
from convex.utils_regularization import __utils_ind_var_scales
from convex.utils_regularization import __utils_dep_var_scale
from convex.utils_regularization import __utils_normalize_data

## ========================================================================

def __ridge_cv_args(schema_madlib, func_args, param_to_try,
                    param_values, data_id,
                    id_is_random, validation_result, fold_num):
    """
    Generate argument list for the __ridge_newton_cv, used in the CV wrapper.
    
    Cross validation (CV) uses a universal interface for all the modules that it supports. Each module should
    provide its own adaptor to convert the inputs from the universal interface to the inputs that the
    module's CV function recognize.

    @param schema_madlib Name of the MADlib schema, properly escaped/quoted
    @param func_args A string list with each item having the form of "arg = value". It gives the arguments
                     needed by the modelling function in the cross validation.
    @param param_to_try The name of the parameter in the modelling function that CV will run through. For Ridge
                        regression, this one must be "lambda"
    @param param_values The different values that CV will run through.
    @param data_id Whether the data table has a unique ID associated with each row. If data_id exists, then
                   it would be be much easier to split the data into training and validation portions. Otherwise,
                   CV function will copy the original data (only copy the part used in the calculation) and add
                   a random ID for each row.
    @param id_is_random If data_id is not None, then whether this ID is randomly assigned to each row. If it is
                        not, then a mapping table will be created, mapping the original non-random ID to a
                        random ID.
    @param validation_result The table name to store the result of CV. It has 3 columns: lamda value, MSE error
                             average, and MSE error standard deviation.
    @param fold_num The cross validation fold number.

    The output is a dict with all parameters and their values for ridge's CV function __ridge_newton_cv
    """
    # ridge regression modelling function accepts the following parameters:
    # tbl_source - the data source table,
    # col_ind_var - independent variable column name,
    # col_dep_var - dependent variable column name,
    # tbl_output - output table of ridge (of no use and will be ignored)
    # lambda - the regularization parameter (of no use and will be ignored)
    # normalization - whether normalize the data (better convergence, consistent with R default and slower speed)
    allowed_args = set(["tbl_source", "col_ind_var", "col_dep_var", "tbl_output", "lambda", "normalization"])
    
    name_value = dict()
    name_value["schema_madlib"] = schema_madlib
    name_value["data_tbl"] = None
    name_value["col_ind_var"] = None
    name_value["col_dep_var"] = None
    name_value["normalization"] = None
    name_value["validation_result"] = validation_result
    name_value["fold_num"] = fold_num
    name_value["upto_fold"] = fold_num
    name_value["data_id"] = data_id
    name_value["id_is_random"] = id_is_random

    # the only parameter that CV can run for ridge regression is lambda
    if param_to_try != "lambda":
        plpy.error("Only lambda can be used to cross-validation in Ridge regression! {0} is not allowed.".format(param_to_try))
    name_value["lambda_values"] = param_values
 
    for s in func_args:
        items = s.split("=")
        if (len(items) != 2):
            plpy.error("Argument list syntax error!")
        arg_name = items[0].strip()
        arg_value = items[1].strip()

        if arg_name not in allowed_args:
            plpy.error("{0} is not a valid argument name for module Ridge.".format(arg_name))

        if arg_name == "tbl_source":
            name_value["data_tbl"] = arg_value
            continue

        if arg_name == "col_ind_var":
            name_value["col_ind_var"] = arg_value
            continue

        if arg_name == "col_dep_var":
            name_value["col_dep_var"] = arg_value
            continue

        if arg_name == "normalization":
            name_value["normalization"] = arg_value
            continue
    
    if name_value["normalization"] is None:
        name_value["normalization"] = False

    if name_value["data_tbl"] is None or name_value["col_ind_var"] is None or name_value["col_dep_var"] is None:
        plpy.error("tbl_source, col_ind_var and col_dep_var must be provided!")

    return name_value

## ========================================================================

def __ridge_newton_cv(**kwargs):
    __ridge_newton_cv_validate_args(kwargs)
    return __ridge_newton_cv_compute(**kwargs)

## ========================================================================

def __ridge_newton_cv_validate_args(args):
    """
    Validate the arguments to __ridge_newton_cv
    """
    if (args["data_tbl"] is None or args["validation_result"] is None
        or args["col_ind_var"] is None or args["col_dep_var"] is None
        or args["lambda_values"] is None or args["fold_num"] is None
        or args["upto_fold"] is None):
        plpy.error("Ridge CV error: You have unsupported Null value(s) in the arguments!")
    
    if not __is_tbl_exists(args["data_tbl"]):
        plpy.error("Ridge CV error: Data table does not exist!")

    if __is_tbl_exists_in_schema(args["validation_result"]):
        plpy.error("Ridge CV error: Output table already exists!")
        
    if not __is_tbl_has_rows(args["data_tbl"]):
        plpy.error("Ridge CV error: Data table is empty!")

    # if not __is_col_exists(args["data_tbl"], [args["col_ind_var"], args["col_dep_var"]]):
    #     plpy.error("Ridge CV error: Some column does not exist!")

    if not __is_scalar_col_no_null(args["data_tbl"], args["col_dep_var"]):
        plpy.error("Ridge CV error: Dependent variable has Null values! Please filter out Null values before using this function!")

    # if not __is_array_col_same_dimension(args["data_tbl"], args["col_ind_var"]):
    #     plpy.error("Ridge CV error: Independent variable arrays have unequal lengths!")

    # if not __is_array_col_no_null(args["data_tbl"], args["col_ind_var"]):
    #     plpy.error("Ridge CV error: Independent variable arrays have Null values! Please filter out Null values before using this function!")

    for lambda_value in args["lambda_values"]:
        if lambda_value < 0:
            plpy.error("Ridge CV error: The regularization parameter lambda cannot be negative!")

    fold_num = args["fold_num"]
    upto_fold = args["upto_fold"]
    if fold_num <= 1:
        plpy.error("Ridge CV error: Cross validation total fold number should be larger than 1!")

    if upto_fold < 1 or upto_fold > fold_num:
        plpy.error("Ridge CV error: Cannot run with cross validation fold smalled than 1 or larger than total fold number!")

    row_num = plpy.execute("select count(*) from " + args["data_tbl"])[0]["count"]
    if row_num <= fold_num:
        plpy.error("Ridge CV error: Too few data! Fewer than fold_num.")

    return None

# ========================================================================
    
def __ridge_newton_cv_compute(**kwargs):
    """
    Cross validation for ridge without lock limits.

    The output is a table which has 3 columns: lamda value, MSE error average,
    and MSE error standard deviation.

    Parameters:
    schema_madlib -- Name of the MADlib schema, properly escaped/quoted
    data_tbl -- Name of data source table
    col_ind_var -- Name of indepdendent variable (an array) column
    col_dep_var -- Name of dependent variable (double) column
    lambda_values -- An array of values for lambda (the regularization parameter)
    data_id -- Whether the data table has a unique ID associated with each row. If data_id exists, then
               it would be be much easier to split the data into training and validation portions. Otherwise,
               CV function will copy the original data (only copy the part used in the calculation) and add
               a random ID for each row.
    id_is_random -- If data_id is not None, then whether this ID is randomly assigned to each row. If it is
                    not, then a mapping table will be created, mapping the original non-random ID to a
                    random ID.
    validation_result -- The table name to store the result of CV. It has 3 columns: lamda value, MSE error
                         average, and MSE error standard deviation.
    fold_num -- The cross validation fold number.
    upto_fold -- Which fold number the function will run up to
    """
    old_msg_level = plpy.execute("select setting from pg_settings where name='client_min_messages'")[0]['setting']
    plpy.execute("set client_min_messages to warning")

    # copy data if needed,
    # update name space,
    # check the validity of fold_num,
    # create table to store all the fitting coefficients
    __utils_cv_preprocess(kwargs)

    upto_fold = kwargs["upto_fold"]
    lambda_values = kwargs["lambda_values"]
    normalization = kwargs["normalization"]
    tbl_accum_error = kwargs["tbl_accum_error"]
    validation_result = kwargs["validation_result"]
    kwargs["col_ind_var_tmp"] = __unique_string()
    kwargs["col_dep_var_tmp"] = __unique_string()

    accum_count = 0
    coef_count = 0
    for k in range(upto_fold):
        # create training and validation sets
        # and normalize the training set if requested by
        # the user (parameter normalization in func_args is True)
        __utils_cv_split_and_normalization(k, kwargs)
        row_num_train = kwargs["row_num_train"]
        
        for value in lambda_values:
            coef_count += 1

            # Re-scaling of lambda
            # so that same results can be produced 
            # as R MASS package's lm.ridge function.
            # glmnet package produces different result,
            # which is due to a bug (but the bug actually does not
            # affect the final optimum fitting result)
            effective_lambda = value * row_num_train
            
            # actually compute the fitting coefficients
            # manually include the intercept as the last fitting parameter
            plpy.execute("""
                insert into {tbl_coef}
                    select {coef_count}, result[1:{dimension}], result[{dimension}+1]
                    from (
                        select {schema_madlib}.__ridge_newton_result(
                            {schema_madlib}.__ridge_newton_step(
                                array_append(({tbl_train}.{col_ind_var_new})::float8[], 1::float8),
                                ({tbl_train}.{col_dep_var_new})::float8,
                                NULL::float8[],
                                ({dimension}+1)::int2,
                                ({effective_lambda})::float8
                            )
                        ) as result
                        from {tbl_train}
                    ) t
            """.format(coef_count = coef_count,
                       effective_lambda = effective_lambda,
                       **kwargs))
       
            if normalization:
                # restore the fitting coefficients to the original scale
                # without introducing new tables
                coef_count += 1
                __utils_normalization_cv_restore(coef_count, **kwargs)

            # directly compute error
            error = plpy.execute("""
                select avg((real_value - pred)^2) as error
                from (
                    select
                        {schema_madlib}.ridge_linear_newton_predict(coef, intercept, z.{col_ind_var_tmp}) as pred,
                        z.{col_dep_var_tmp} as real_value
                    from
                        {tbl_coef},
                        (
                            select
                                {col_dep_var} as {col_dep_var_tmp},
                                {col_ind_var_cp_new} as {col_ind_var_tmp}
                            from {tbl_valid}
                        ) z
                    where {tbl_coef}.id = {coef_count}
                ) t
            """.format(coef_count = coef_count, **kwargs))[0]["error"]

            # append error into one table
            accum_count += 1
            __utils_accumulate_error(accum_count, tbl_accum_error, value, error)

    # for each lambda value, compute the average and standard deviation of errors
    __cv_summarize_result(tbl_accum_error, validation_result, "lambda", False, kwargs["schema_madlib"])

    # clean up the temporary tables
    plpy.execute("""
        drop table if exists {tbl_all_data};
        drop table if exists {tbl_train};
        drop table if exists {tbl_inter};
        drop table if exists {tbl_valid};
        drop table if exists {tbl_random_id};
        drop table if exists {tbl_coef};
        drop table if exists {tbl_ind_scales};
        drop table if exists {tbl_dep_scale};
        drop table if exists {tbl_accum_error}
    """.format(**kwargs))

    plpy.execute("set client_min_messages to " + old_msg_level)

    return None

## ========================================================================

def __ridge_newton_cv_help():
    return """
    Allowed modelling function arguments:

    * tbl_source - the data source table,
    * col_ind_var - independent variable column name,
    * col_dep_var - dependent variable column name,
    * tbl_output - output table of ridge (as an intermediate table of CV, it will be ignored if is given here)
    * lambda - the regularization parameter (in CV lambda is given as the parameter values to try, and any value given here will be ignored)
    * normalization - whether normalize the data (better convergence, consistent with R default and slower speed)

    Usage example:

    select madlib.cross_validation(
        'ridge',
        '{normalization = True, col_ind_var = val, col_dep_var = dep, tbl_source = cvtest}',
        'lambda',
        '{0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2}'::double precision[],
        NULL::varchar, -- no unique ID associated with rows
        False,  -- if there is a unique ID, whether it is random
        'validation_result',
        10    -- cross validation fold
    );
    """

## ========================================================================
## ========================================================================
## ========================================================================

def ridge_newton_train (schema_madlib, tbl_source, col_ind_var,
                        col_dep_var, tbl_output, lambda_value,
                        normalization, **kwargs):
    (tbl_source, col_ind_var,
     col_dep_var, tbl_output,
     lambda_value, normalization) = __ridge_validate_args(tbl_source, col_ind_var,
                                                          col_dep_var, tbl_output,
                                                          lambda_value, normalization)
    
    return ridge_newton_train_compute (schema_madlib, tbl_source, col_ind_var,
                                       col_dep_var, tbl_output, lambda_value,
                                       normalization, **kwargs)

## ========================================================================

def __ridge_validate_args(tbl_source, col_ind_var,
                          col_dep_var, tbl_output,
                          lambda_value, normalization):
    if (tbl_source is None or col_ind_var is None or col_dep_var is None
        or tbl_output is None or lambda_value is None or normalization is None):
        plpy.error("Ridge error: You have unsupported Null value(s) in arguments!")
    
    if not __is_tbl_exists(tbl_source):
        plpy.error("Ridge error: Data table " + tbl_source + " does not exist!")

    if __is_tbl_exists_in_schema(tbl_output):
        plpy.error("Ridge error: Output table " + tbl_output + " already exists!")

    if not __is_tbl_has_rows(tbl_source):
        plpy.error("Ridge error: Data table " + tbl_source + " is empty!")

    # if not __is_col_exists(tbl_source, [col_ind_var, col_dep_var]):
    #     plpy.error("Ridge error: Some column does not exist!")

    if not __is_scalar_col_no_null(tbl_source, col_dep_var):
        plpy.error("Ridge error: Dependent variable has Null values! Please filter out Null values before using this function!")

    # if not __is_array_col_same_dimension(tbl_source, col_ind_var):
    #     plpy.error("Ridge error: Independent variable arrays have unequal lengths!")

    # if not __is_array_col_no_null(tbl_source, col_ind_var):
    #     plpy.error("Ridge error: Independent variable arrays have Null values! Please filter out Null values before using this function!")

    if lambda_value < 0:
        plpy.error("Ridge error: The regularization parameter lambda cannot be negative!")

    return (tbl_source, col_ind_var,
            col_dep_var, tbl_output,
            lambda_value, normalization)
                   
    
def ridge_newton_train_compute (schema_madlib, tbl_source, col_ind_var,
                                col_dep_var, tbl_output, lambda_value,
                                normalization, **kwargs):
    """
    @param tbl_source -- Name of data source table
    @param col_ind_var -- Name of indepdendent variable (an array) column
    @param col_dep_var -- Name of dependent variable (double) column
    @param tbl_output -- Name of tabel to store results
    @param lambda_value -- Value of the regularization parameter
    @param normalization -- Whether to normalize the dependent variables
    """
    old_msg_level = plpy.execute("select setting from pg_settings where name='client_min_messages'")[0]['setting']
    plpy.execute("set client_min_messages to warning")

    # __validation_arguments()

    args = dict(schema_madlib = schema_madlib, tbl_source = tbl_source,
                col_ind_var = col_ind_var, col_dep_var = col_dep_var,
                tbl_output = tbl_output, lambda_value = lambda_value,
                tbl_ind_scales = __unique_string(),
                tbl_dep_scale = __unique_string(),
                tbl_data_scaled = __unique_string(),
                col_ind_var_norm_new = __unique_string(),
                col_ind_var_tmp = __unique_string(),
                col_dep_var_norm_new = __unique_string(),
                col_dep_var_tmp = __unique_string())
    
    row_num = plpy.execute("select count(*) from " + tbl_source)[0]["count"]
    dimension = plpy.execute("""
                             select max(array_upper({col_ind_var},1)) as d
                             from {tbl_source}
                             """.format(**args))[0]["d"]
    args.update(dimension = dimension, row_num = row_num)
    
    if normalization:
        __utils_ind_var_scales(tbl_data = tbl_source, col_ind_var = col_ind_var, row_num = row_num,
                               dimension = dimension, tbl_scales = args["tbl_ind_scales"])
        __utils_dep_var_scale(tbl_data = tbl_source, col_dep_var = col_dep_var, 
                              row_num = row_num, tbl_scale = args["tbl_dep_scale"])
        __utils_normalize_data(tbl_data = tbl_source, col_ind_var = col_ind_var, dimension = dimension,
                               col_dep_var = col_dep_var, tbl_ind_scales = args["tbl_ind_scales"],
                               tbl_dep_scale = args["tbl_dep_scale"], tbl_data_scaled = args["tbl_data_scaled"],
                               col_ind_var_norm_new = args["col_ind_var_norm_new"],
                               col_dep_var_norm_new = args["col_dep_var_norm_new"])
        tbl_inter = args["tbl_data_scaled"]
        use_temp = "temp"
        tbl_inter_result = __unique_string()
        args["col_ind_var_new"] = args["col_ind_var_norm_new"]
        args["col_dep_var_new"] = args["col_dep_var_norm_new"]
    else:
        tbl_inter = tbl_source
        tbl_inter_result = tbl_output
        use_temp = ""
        args["col_ind_var_new"] = col_ind_var
        args["col_dep_var_new"] = col_dep_var

    args.update(tbl_inter = tbl_inter, tbl_inter_result = tbl_inter_result,
                use_temp = use_temp)

    plpy.execute("""
                 drop table if exists {tbl_inter_result};
                 create {use_temp} table {tbl_inter_result} (
                    coefficients    double precision[],
                    intercept       double precision,
                    log_likelihood  double precision,
                    normalization   boolean)
                 """.format(**args))

    plpy.execute("""
                 insert into {tbl_inter_result}
                 select result[1:{dimension}], result[{dimension}+1], 0, False
                 from (
                    select {schema_madlib}.__ridge_newton_result(
                        {schema_madlib}.__ridge_newton_step(
                            array_append(({col_ind_var_new})::double precision[],1::double precision),
                            ({col_dep_var_new})::double precision,
                            (NULL)::double precision[],
                            ({dimension}+1)::int2,
                            {effective_lambda}::double precision)
                       ) as result
                    from {tbl_inter}
                 ) t                 
                 """.format(effective_lambda = lambda_value * row_num,
                            **args))

    # update the log likelihood
    plpy.execute("""
                 update {tbl_inter_result} set log_likelihood = ll from
                 (
                    select -(loss + module_2) as ll
                    from
                        (
                           select
                               avg((w.{col_dep_var} - {schema_madlib}.ridge_linear_newton_predict(z.coefficients, z.intercept, w.{col_ind_var_tmp}))^2) / 2. as loss
                           from
                               {tbl_inter_result} as z,
                               (
                                   select
                                       {col_dep_var} as {col_dep_var_new},
                                       {col_ind_var_new} as {col_ind_var_tmp}
                                   from {tbl_inter}
                               ) w
                        ) s,
                        (
                           select {lambda_value} * sum(coef^2) *0.5 as module_2
                           from (
                              select unnest(coefficients) as coef
                              from {tbl_inter_result}
                           ) t1                 
                         ) t
                 ) t2
                 """.format(**args))

    if normalization:
        __utils_restore_linear_coef_scales(tbl_coef = tbl_inter_result,
                                           col_coef = 'coefficients',
                                           col_others = ["log_likelihood", "normalization"],
                                           dimension = dimension,
                                           tbl_ind_scales = args["tbl_ind_scales"],
                                           tbl_dep_scale = args["tbl_dep_scale"],
                                           tbl_origin_coef = tbl_output)
        plpy.execute("update {tbl_output} set normalization = True".format(**args))
        plpy.execute("drop table if exists {tbl_inter_result}".format(**args))

    plpy.execute("""
                 drop table if exists {tbl_ind_scales};
                 drop table if exists {tbl_dep_scale};
                 drop table if exists {tbl_data_scaled};
                 """.format(**args))
        
    plpy.execute("set client_min_messages to " + old_msg_level)
    return None
